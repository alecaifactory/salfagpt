# Gu√≠a de Migraci√≥n de Datos: GCP ‚Üí AWS
## Firestore to DynamoDB, Cloud Storage to S3

**Fecha:** 24 de Noviembre, 2025  
**Versi√≥n:** 1.0  
**Prop√≥sito:** Migraci√≥n segura de datos existentes

---

## üéØ Objetivo

Migrar datos existentes de cartolas bancarias desde **Google Cloud Platform** (Firestore + Cloud Storage) a **AWS** (DynamoDB + S3) manteniendo:
- ‚úÖ 100% integridad de datos
- ‚úÖ Cero p√©rdida de informaci√≥n
- ‚úÖ M√≠nimo downtime
- ‚úÖ Rollback disponible

---

## üìä Inventario de Datos

### Datos en GCP (Actual)

**Firestore Collections:**
```
Collection: context_sources (o cartola_extractions si existe)
‚îú‚îÄ‚îÄ Documentos: ~X documentos
‚îú‚îÄ‚îÄ Campos:
‚îÇ   ‚îú‚îÄ‚îÄ id: string
‚îÇ   ‚îú‚îÄ‚îÄ userId: string
‚îÇ   ‚îú‚îÄ‚îÄ organizationId: string (opcional)
‚îÇ   ‚îú‚îÄ‚îÄ type: 'pdf' | 'csv' | ...
‚îÇ   ‚îú‚îÄ‚îÄ extractedData: string (JSON como texto)
‚îÇ   ‚îú‚îÄ‚îÄ metadata: object
‚îÇ   ‚îú‚îÄ‚îÄ createdAt: Timestamp
‚îÇ   ‚îî‚îÄ‚îÄ updatedAt: Timestamp
‚îî‚îÄ‚îÄ Tama√±o estimado: ~Y GB
```

**Cloud Storage Buckets:**
```
Bucket: gen-lang-client-0986191192-uploads (o similar)
‚îú‚îÄ‚îÄ Archivos PDF: ~Z archivos
‚îú‚îÄ‚îÄ Tama√±o total: ~W GB
‚îú‚îÄ‚îÄ Retenci√≥n: 7 d√≠as (lifecycle)
‚îî‚îÄ‚îÄ Ubicaci√≥n: us-central1
```

---

## üîÑ Estrategia de Migraci√≥n

### Opci√≥n 1: Migraci√≥n Limpia (RECOMENDADO)

**Descripci√≥n:** Iniciar AWS Lambda sin migrar datos hist√≥ricos de GCP.

**Justificaci√≥n:**
- ‚úÖ **Retention corta**: Datos GCP se auto-eliminan en 7-90 d√≠as (lifecycle policies)
- ‚úÖ **Datos no cr√≠ticos**: Extracciones hist√≥ricas no necesarias para operaci√≥n
- ‚úÖ **Simplicidad**: Sin complejidad de migraci√≥n
- ‚úÖ **Zero risk**: No hay forma de corromper datos

**Proceso:**
```
D√≠a 0: Sistema GCP funcional
       ‚Üì
D√≠a 1-14: Desarrollo AWS Lambda (paralelo, GCP sigue funcionando)
       ‚Üì
D√≠a 15: Deploy AWS Lambda staging
       ‚Üì (GCP sigue activo)
D√≠a 15-21: Testing staging
       ‚Üì
D√≠a 22: Deploy AWS Lambda producci√≥n
       ‚Üì (switch tr√°fico a AWS)
D√≠a 22+: GCP en modo lectura (mantener 90 d√≠as como backup)
       ‚Üì
D√≠a 112: Desactivar GCP completamente
```

**Pros:**
- ‚úÖ Sin complejidad de migraci√≥n
- ‚úÖ Sin riesgo de p√©rdida de datos
- ‚úÖ Implementaci√≥n r√°pida
- ‚úÖ GCP como backup durante transici√≥n

**Contras:**
- ‚ö†Ô∏è Datos hist√≥ricos GCP no accesibles desde AWS (aceptable)
- ‚ö†Ô∏è Usuarios no ven extracciones antiguas en nuevo sistema (temporal)

---

### Opci√≥n 2: Migraci√≥n Completa de Datos

**Descripci√≥n:** Migrar todos los datos hist√≥ricos de GCP a AWS.

**Solo si:**
- Tienes >1,000 extracciones hist√≥ricas que usuarios necesitan
- Compliance requiere mantener hist√≥rico
- Analytics requiere datos hist√≥ricos

**Proceso:**

#### Fase 1: Exportar de Firestore

```bash
# 1. Exportar colecci√≥n completa
gcloud firestore export gs://salfagpt-firestore-export/cartola-export-$(date +%Y%m%d) \
  --collection-ids=context_sources \
  --project=salfagpt

# O query espec√≠fica (solo cartolas)
# Si diferencias context_sources de cartolas por campo 'type'

# 2. Descargar export
gsutil -m cp -r gs://salfagpt-firestore-export/cartola-export-* ./firestore-export/

# 3. Convertir a formato JSON legible
npm install -g @google-cloud/firestore-export
firestore-export --accountCredentials ./service-account-key.json \
  --nodePath 'context_sources' \
  --outputFile ./data/firestore-cartolas.json
```

#### Fase 2: Transformar Datos

```javascript
// scripts/transform-firestore-to-dynamodb.js

const fs = require('fs');

// 1. Leer export de Firestore
const firestoreData = JSON.parse(fs.readFileSync('./data/firestore-cartolas.json', 'utf8'));

// 2. Transformar a formato DynamoDB
const dynamoItems = Object.entries(firestoreData).map(([docId, docData]) => {
  return {
    PutRequest: {
      Item: {
        id: docId,
        userId: docData.userId,
        organizationId: docData.organizationId || null,
        status: 'completed',  // Asumimos completados
        fileName: docData.metadata?.originalFileName || 'unknown.pdf',
        fileSize: docData.metadata?.originalFileSize || 0,
        s3Key: null,  // No hay archivo en S3 (expirado)
        extractionResult: docData.extractedData ? JSON.parse(docData.extractedData) : null,
        createdAt: docData.createdAt?._seconds ? docData.createdAt._seconds * 1000 : Date.now(),
        updatedAt: docData.updatedAt?._seconds ? docData.updatedAt._seconds * 1000 : Date.now(),
        completedAt: docData.createdAt?._seconds ? docData.createdAt._seconds * 1000 : Date.now(),
        ttl: Math.floor(Date.now() / 1000) + (90 * 24 * 60 * 60),  // 90 d√≠as desde ahora
        // Metadata para trazabilidad
        _migrated: true,
        _migratedFrom: 'gcp-firestore',
        _migrationDate: Date.now()
      }
    }
  };
});

// 3. Dividir en batches de 25 (l√≠mite DynamoDB BatchWriteItem)
const batches = [];
for (let i = 0; i < dynamoItems.length; i += 25) {
  batches.push(dynamoItems.slice(i, i + 25));
}

// 4. Guardar batches
fs.writeFileSync('./data/dynamodb-batches.json', JSON.stringify(batches, null, 2));

console.log(`‚úÖ Transformed ${dynamoItems.length} documents into ${batches.length} batches`);
```

#### Fase 3: Importar a DynamoDB

```javascript
// scripts/import-to-dynamodb.js

const AWS = require('aws-sdk');
const fs = require('fs');

AWS.config.update({ region: 'us-east-1' });
const dynamoDB = new AWS.DynamoDB.DocumentClient();

async function importBatches() {
  const batches = JSON.parse(fs.readFileSync('./data/dynamodb-batches.json', 'utf8'));
  
  console.log(`üì¶ Importing ${batches.length} batches...`);
  
  for (let i = 0; i < batches.length; i++) {
    const batch = batches[i];
    
    try {
      await dynamoDB.batchWrite({
        RequestItems: {
          [process.env.DYNAMODB_TABLE]: batch
        }
      }).promise();
      
      console.log(`‚úÖ Batch ${i + 1}/${batches.length} imported`);
      
      // Rate limiting (DynamoDB free tier: 25 writes/sec)
      await new Promise(resolve => setTimeout(resolve, 200));
      
    } catch (error) {
      console.error(`‚ùå Batch ${i + 1} failed:`, error);
      
      // Guardar batch fallido para retry
      fs.appendFileSync('./data/failed-batches.json', JSON.stringify(batch) + '\n');
    }
  }
  
  console.log('üéâ Import complete!');
}

importBatches().catch(console.error);

// Ejecutar:
// DYNAMODB_TABLE=cartola_extractions_staging node scripts/import-to-dynamodb.js
```

#### Fase 4: Migrar Archivos S3 (Opcional)

**Solo si archivos <7 d√≠as y necesitas preservarlos:**

```bash
# 1. Listar archivos en Cloud Storage
gsutil ls -r gs://salfagpt-uploads/cartola/ > ./data/gcs-files.txt

# 2. Copiar a S3 (con gsutil y AWS CLI)
while read gcs_path; do
  # Descargar de GCS
  gsutil cp "$gcs_path" ./temp/file.pdf
  
  # Subir a S3
  filename=$(basename "$gcs_path")
  aws s3 cp ./temp/file.pdf s3://nubox-cartola-uploads-prod/migrated/$filename
  
  echo "‚úÖ Migrated: $filename"
done < ./data/gcs-files.txt

# O usar Cloud Storage Transfer Service:
# https://cloud.google.com/storage-transfer/docs/s3-transfer
```

**Alternativa: gsutil rsync (m√°s r√°pido)**
```bash
# Instalar boto para S3
pip install boto

# Configurar boto con AWS credentials
cat > ~/.boto << EOF
[Credentials]
aws_access_key_id = YOUR_AWS_KEY
aws_secret_access_key = YOUR_AWS_SECRET

[s3]
host = s3.us-east-1.amazonaws.com
EOF

# Sync GCS ‚Üí S3
gsutil -m rsync -r gs://salfagpt-uploads/cartola/ s3://nubox-cartola-uploads-prod/migrated/
```

---

## üîí Seguridad Durante Migraci√≥n

### Encriptaci√≥n de Datos en Tr√°nsito

```bash
# 1. Exportar de Firestore encriptado
gcloud firestore export gs://salfagpt-export/encrypted-$(date +%Y%m%d) \
  --collection-ids=context_sources

# 2. Descargar y encriptar localmente
gsutil -m cp -r gs://salfagpt-export/encrypted-* ./export/
tar -czf export.tar.gz ./export/
gpg --symmetric --cipher-algo AES256 export.tar.gz

# 3. Subir a S3 encriptado
aws s3 cp export.tar.gz.gpg s3://nubox-migration-bucket/firestore-export/ \
  --server-side-encryption AES256

# 4. Procesar en Lambda con decryption
```

### Validaci√≥n de Integridad

```javascript
// scripts/validate-migration.js

const AWS = require('aws-sdk');
const fs = require('fs');

const dynamoDB = new AWS.DynamoDB.DocumentClient();

async function validateMigration() {
  // 1. Contar documentos originales (Firestore)
  const firestoreData = JSON.parse(fs.readFileSync('./data/firestore-cartolas.json', 'utf8'));
  const firestoreCount = Object.keys(firestoreData).length;
  
  console.log(`üìä Firestore documents: ${firestoreCount}`);
  
  // 2. Contar documentos migrados (DynamoDB)
  const scanResult = await dynamoDB.scan({
    TableName: process.env.DYNAMODB_TABLE,
    Select: 'COUNT',
    FilterExpression: 'attribute_exists(#migrated)',
    ExpressionAttributeNames: {
      '#migrated': '_migrated'
    }
  }).promise();
  
  const dynamoCount = scanResult.Count;
  
  console.log(`üìä DynamoDB documents (migrated): ${dynamoCount}`);
  
  // 3. Validar match
  if (firestoreCount === dynamoCount) {
    console.log('‚úÖ Migration count matches!');
  } else {
    console.error(`‚ùå Mismatch: ${firestoreCount} vs ${dynamoCount}`);
    console.error(`   Missing: ${firestoreCount - dynamoCount} documents`);
  }
  
  // 4. Validar estructura de algunos documentos
  const sampleIds = Object.keys(firestoreData).slice(0, 10);
  
  for (const id of sampleIds) {
    const firestoreDoc = firestoreData[id];
    
    const dynamoResult = await dynamoDB.get({
      TableName: process.env.DYNAMODB_TABLE,
      Key: { id }
    }).promise();
    
    const dynamoDoc = dynamoResult.Item;
    
    if (!dynamoDoc) {
      console.error(`‚ùå Document ${id} missing in DynamoDB`);
      continue;
    }
    
    // Validar campos cr√≠ticos
    if (firestoreDoc.userId !== dynamoDoc.userId) {
      console.error(`‚ùå userId mismatch for ${id}`);
    }
    
    console.log(`‚úÖ Document ${id} validated`);
  }
  
  console.log('üéâ Validation complete!');
}

validateMigration().catch(console.error);
```

---

## üîÑ Migraci√≥n Sin Downtime

### Estrategia Blue-Green

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               MIGRACI√ìN BLUE-GREEN                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  Semana 1-2: Preparaci√≥n                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  GCP (BLUE)      ‚îÇ  ‚Üê 100% tr√°fico                   ‚îÇ
‚îÇ  ‚îÇ  Cloud Run       ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ  Firestore       ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ  Cloud Storage   ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  AWS (GREEN)     ‚îÇ  ‚Üê 0% tr√°fico (staging)           ‚îÇ
‚îÇ  ‚îÇ  Lambda          ‚îÇ  ‚Üê Desarrollo y testing           ‚îÇ
‚îÇ  ‚îÇ  DynamoDB        ‚îÇ  ‚Üê Sin datos prod a√∫n            ‚îÇ
‚îÇ  ‚îÇ  S3              ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Semana 3: Canary Release                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  GCP (BLUE)      ‚îÇ  ‚Üê 90% tr√°fico                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  AWS (GREEN)     ‚îÇ  ‚Üê 10% tr√°fico (canary)           ‚îÇ
‚îÇ  ‚îÇ  - Monitor errors‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ  - Monitor latency                                   ‚îÇ
‚îÇ  ‚îÇ  - Compare quality                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  D√≠a 1-2: Si 10% exitoso                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  GCP (BLUE)      ‚îÇ  ‚Üê 50% tr√°fico                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  AWS (GREEN)     ‚îÇ  ‚Üê 50% tr√°fico                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  D√≠a 3-5: Si 50% exitoso                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  GCP (BLUE)      ‚îÇ  ‚Üê 0% tr√°fico (backup pasivo)     ‚îÇ
‚îÇ  ‚îÇ  - Mantener 90 d√≠as                                  ‚îÇ
‚îÇ  ‚îÇ  - Solo lectura                                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  AWS (GREEN)     ‚îÇ  ‚Üê 100% tr√°fico ‚úÖ                ‚îÇ
‚îÇ  ‚îÇ  - Producci√≥n     ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ  - Fully active   ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  D√≠a 90+: Cleanup GCP                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  GCP (BLUE)      ‚îÇ  ‚Üê Desactivado ‚ôªÔ∏è                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Doble Escritura (Transici√≥n)

**Si requieres mantener ambos sistemas sincronizados (temporal):**

```javascript
// Durante transici√≥n, escribir a ambos
async function saveExtraction(data) {
  const results = await Promise.allSettled([
    // Write to GCP (existing)
    saveToFirestore(data),
    
    // Write to AWS (new)
    saveToDynamoDB(data)
  ]);
  
  // Log results
  results.forEach((result, idx) => {
    const system = idx === 0 ? 'GCP' : 'AWS';
    if (result.status === 'fulfilled') {
      console.log(`‚úÖ ${system} write succeeded`);
    } else {
      console.error(`‚ùå ${system} write failed:`, result.reason);
    }
  });
  
  // Require at least one success
  if (results.every(r => r.status === 'rejected')) {
    throw new Error('Both GCP and AWS writes failed');
  }
}

// Despu√©s de validar AWS funciona:
// 1. Desactivar writes a GCP
// 2. Solo leer de GCP si no existe en AWS (fallback)
// 3. Eventualmente eliminar c√≥digo GCP
```

---

## üì¶ Script de Migraci√≥n Completo

### migrate-all.sh

```bash
#!/bin/bash
# Migraci√≥n completa GCP ‚Üí AWS
# Uso: ./migrate-all.sh [staging|prod]

set -e

STAGE=${1:-staging}
TIMESTAMP=$(date +%Y%m%d-%H%M%S)

echo "üöÄ Starting GCP ‚Üí AWS migration"
echo "================================"
echo "Stage: $STAGE"
echo "Timestamp: $TIMESTAMP"
echo ""

# ============================================================================
# PASO 1: Backup GCP
# ============================================================================
echo "üì¶ Step 1: Backup GCP data"

# Export Firestore
echo "  Exporting Firestore..."
gcloud firestore export gs://salfagpt-backup/migration-$TIMESTAMP \
  --collection-ids=context_sources \
  --project=salfagpt

# List Cloud Storage files
echo "  Listing Cloud Storage files..."
gsutil ls -r gs://salfagpt-uploads/cartola/ > ./migration-data/gcs-files-$TIMESTAMP.txt

echo "  ‚úÖ GCP backup complete"
echo ""

# ============================================================================
# PASO 2: Crear Infraestructura AWS (si no existe)
# ============================================================================
echo "üìã Step 2: Setup AWS infrastructure"

# Crear S3 bucket
aws s3 mb s3://nubox-cartola-uploads-$STAGE --region us-east-1 2>/dev/null || echo "  Bucket already exists"

# Crear DynamoDB table (via Serverless Framework)
cd lambda/
serverless deploy --stage $STAGE

echo "  ‚úÖ AWS infrastructure ready"
echo ""

# ============================================================================
# PASO 3: Transformar Datos
# ============================================================================
echo "üîÑ Step 3: Transform data"

# Descargar Firestore export
gsutil -m cp -r gs://salfagpt-backup/migration-$TIMESTAMP ./migration-data/

# Convertir a JSON
node scripts/transform-firestore-to-dynamodb.js

echo "  ‚úÖ Data transformed"
echo ""

# ============================================================================
# PASO 4: Importar a DynamoDB
# ============================================================================
echo "üì• Step 4: Import to DynamoDB"

DYNAMODB_TABLE=cartola_extractions_$STAGE node scripts/import-to-dynamodb.js

echo "  ‚úÖ DynamoDB import complete"
echo ""

# ============================================================================
# PASO 5: Validar Migraci√≥n
# ============================================================================
echo "‚úÖ Step 5: Validate migration"

DYNAMODB_TABLE=cartola_extractions_$STAGE node scripts/validate-migration.js

echo "  ‚úÖ Validation complete"
echo ""

# ============================================================================
# PASO 6: Opcional - Migrar Archivos S3
# ============================================================================
read -p "¬øMigrar archivos de Cloud Storage a S3? (y/n): " MIGRATE_FILES

if [ "$MIGRATE_FILES" = "y" ]; then
  echo "üìÅ Step 6: Migrate files to S3"
  
  # Sync GCS ‚Üí S3
  gsutil -m rsync -r gs://salfagpt-uploads/cartola/ s3://nubox-cartola-uploads-$STAGE/migrated/
  
  echo "  ‚úÖ Files migrated"
else
  echo "  ‚è≠Ô∏è  Skipping file migration"
fi

echo ""
echo "üéâ Migration complete!"
echo ""
echo "Next steps:"
echo "  1. Test API endpoints in $STAGE"
echo "  2. Verify data integrity"
echo "  3. Monitor for errors"
echo "  4. If all OK, proceed to production"
```

---

## ‚úÖ Validaci√≥n Post-Migraci√≥n

### Checklist de Validaci√≥n

```bash
# 1. Verificar conteo de documentos
FIRESTORE_COUNT=$(cat ./data/firestore-cartolas.json | jq 'keys | length')
DYNAMO_COUNT=$(aws dynamodb scan \
  --table-name cartola_extractions_staging \
  --select COUNT \
  --filter-expression "attribute_exists(#migrated)" \
  --expression-attribute-names '{"#migrated": "_migrated"}' \
  --query 'Count' \
  --output text)

echo "Firestore: $FIRESTORE_COUNT"
echo "DynamoDB: $DYNAMO_COUNT"

if [ "$FIRESTORE_COUNT" -eq "$DYNAMO_COUNT" ]; then
  echo "‚úÖ Count matches"
else
  echo "‚ùå Count mismatch!"
fi

# 2. Verificar algunos documentos espec√≠ficos
# Sample 10 random documents
SAMPLE_IDS=$(cat ./data/firestore-cartolas.json | jq -r 'keys | .[:10] | .[]')

for id in $SAMPLE_IDS; do
  # Get from DynamoDB
  DYNAMO_DOC=$(aws dynamodb get-item \
    --table-name cartola_extractions_staging \
    --key "{\"id\": {\"S\": \"$id\"}}" \
    --query 'Item' \
    --output json)
  
  if [ "$DYNAMO_DOC" != "null" ]; then
    echo "‚úÖ Document $id exists in DynamoDB"
  else
    echo "‚ùå Document $id MISSING in DynamoDB"
  fi
done

# 3. Verificar archivos S3 (si se migraron)
S3_COUNT=$(aws s3 ls s3://nubox-cartola-uploads-staging/migrated/ --recursive | wc -l)
echo "S3 files: $S3_COUNT"

# 4. Test funcional
echo "Testing Lambda with migrated data..."
serverless invoke -f processCartola --stage staging --data '{
  "body": "{\"test\": true}"
}'
```

---

## üîô Plan de Rollback

### Si Migraci√≥n Falla

**Paso 1: Detener Escrituras a AWS**
```bash
# Pausar Lambda functions
aws lambda put-function-concurrency \
  --function-name nubox-cartola-extraction-$STAGE-processCartola \
  --reserved-concurrent-executions 0

# O eliminar API Gateway endpoints
serverless remove --stage $STAGE
```

**Paso 2: Restaurar desde Backup**
```bash
# Si necesitas restaurar Firestore
gcloud firestore import gs://salfagpt-backup/migration-$TIMESTAMP \
  --project=salfagpt
```

**Paso 3: Redireccionar Tr√°fico a GCP**
```bash
# GCP Cloud Run debe seguir activo durante transici√≥n
# Simplemente dejar de usar AWS endpoints
```

**Paso 4: Limpieza AWS (opcional)**
```bash
# Eliminar DynamoDB table
aws dynamodb delete-table --table-name cartola_extractions_$STAGE

# Vaciar y eliminar S3 bucket
aws s3 rm s3://nubox-cartola-uploads-$STAGE --recursive
aws s3 rb s3://nubox-cartola-uploads-$STAGE

# Eliminar Lambda functions
serverless remove --stage $STAGE
```

---

## üìä Monitoreo Durante Migraci√≥n

### M√©tricas Clave

**Durante Canary (10% tr√°fico AWS):**
```bash
# 1. Error rate comparison
GCP_ERRORS=$(gcloud logging read "severity=ERROR AND resource.labels.service_name=cloud-run-service" --limit 1000 --format json | jq '. | length')
AWS_ERRORS=$(aws cloudwatch get-metric-statistics \
  --namespace AWS/Lambda \
  --metric-name Errors \
  --dimensions Name=FunctionName,Value=nubox-cartola-extraction-staging-processCartola \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 3600 \
  --statistics Sum \
  --query 'Datapoints[0].Sum' \
  --output text)

echo "GCP errors (last hour): $GCP_ERRORS"
echo "AWS errors (last hour): $AWS_ERRORS"

# 2. Latency comparison
# (Implementar con timestamps en logs)

# 3. Quality comparison
# (Comparar average_extraction_proximity_pct)
```

### Dashboard Comparativo

```javascript
// CloudWatch custom metrics para comparaci√≥n
const cloudwatch = new AWS.CloudWatch();

await cloudwatch.putMetricData({
  Namespace: 'NuboxCartolaMigration',
  MetricData: [
    {
      MetricName: 'TrafficPercentage',
      Value: 10,  // 10% en AWS
      Unit: 'Percent',
      Timestamp: new Date(),
      Dimensions: [
        { Name: 'Environment', Value: 'production' },
        { Name: 'Provider', Value: 'AWS' }
      ]
    },
    {
      MetricName: 'ExtractionAccuracy',
      Value: 95.5,
      Unit: 'Percent',
      Dimensions: [
        { Name: 'Provider', Value: 'AWS' }
      ]
    }
  ]
}).promise();

// Crear dashboard con m√©tricas lado a lado GCP vs AWS
```

---

## üéØ Recomendaci√≥n Final de Migraci√≥n

### Opci√≥n Recomendada: Migraci√≥n Limpia (Sin Migrar Datos)

**Por qu√©:**

1. **Datos Temporales** ‚úÖ
   - Retention: 7-90 d√≠as
   - No es hist√≥rico cr√≠tico
   - Auto-eliminaci√≥n configurada

2. **Simplicidad** ‚úÖ
   - Sin scripts de migraci√≥n complejos
   - Sin riesgo de corrupci√≥n de datos
   - Sin tiempo adicional de desarrollo

3. **Costo-Beneficio** ‚úÖ
   - Migraci√≥n de datos: 1-2 d√≠as adicionales
   - Beneficio: M√≠nimo (datos se eliminan solos)
   - ROI negativo para migrar hist√≥rico

4. **Backup Disponible** ‚úÖ
   - GCP mantiene datos 90 d√≠as
   - Rollback inmediato si problemas AWS
   - Sin p√©rdida de datos

### Proceso Recomendado

```
1. Desarrollar AWS Lambda (2 semanas)
2. Deploy a staging, validar (1 semana)
3. Deploy a producci√≥n con canary (10% ‚Üí 50% ‚Üí 100%)
4. Mantener GCP como backup pasivo (90 d√≠as)
5. Desactivar GCP despu√©s de validaci√≥n completa
6. Datos GCP se auto-eliminan por lifecycle policy
```

**Tiempo total:** 3 semanas  
**Riesgo:** Bajo  
**Complejidad:** Baja  
**Costo:** M√≠nimo

---

## üìö Comandos √ötiles

### Verificar Datos GCP

```bash
# Contar documentos en Firestore
gcloud firestore databases list --project=salfagpt

# Query ejemplo
npx tsx -e "
import { firestore } from './src/lib/firestore.js';
const snapshot = await firestore.collection('context_sources')
  .where('type', '==', 'pdf')
  .limit(10)
  .get();
console.log('Documents:', snapshot.size);
process.exit(0);
"

# Listar archivos Cloud Storage
gsutil ls -lh gs://salfagpt-uploads/cartola/ | head -20
```

### Verificar Datos AWS

```bash
# Contar items en DynamoDB
aws dynamodb scan \
  --table-name cartola_extractions_staging \
  --select COUNT \
  --query 'Count' \
  --output text

# Query por usuario
aws dynamodb query \
  --table-name cartola_extractions_staging \
  --index-name userId-createdAt-index \
  --key-condition-expression "userId = :userId" \
  --expression-attribute-values '{":userId": {"S": "user-test-123"}}' \
  --limit 10

# Listar archivos S3
aws s3 ls s3://nubox-cartola-uploads-staging/uploads/ --recursive --human-readable | head -20
```

---

## ‚úÖ Conclusi√≥n

### Estrategia Recomendada

**‚úÖ MIGRACI√ìN LIMPIA (Sin migrar datos hist√≥ricos)**

**Razones:**
- Datos temporales (7-90 d√≠as retention)
- Simplicidad (sin scripts de migraci√≥n)
- Riesgo cero (GCP como backup 90 d√≠as)
- Tiempo √≥ptimo (sin overhead de migraci√≥n)

**Proceso:**
1. Desarrollar AWS Lambda (c√≥digo ya validado)
2. Deploy staging y validar
3. Deploy producci√≥n con canary
4. GCP como backup pasivo
5. Desactivar GCP despu√©s de 90 d√≠as

**Timeline:** 3 semanas  
**Complejidad:** Baja  
**Riesgo:** Bajo  
**Costo:** M√≠nimo

---

**Si se requiere migraci√≥n completa de datos hist√≥ricos:**
- Seguir Opci√≥n 2 en este documento
- Tiempo adicional: 1-2 d√≠as
- Usar scripts proporcionados
- Validar integridad antes de switch

---

**Siguiente Paso:** Iniciar desarrollo AWS Lambda seg√∫n `GUIA_IMPLEMENTACION_AWS_LAMBDA.md`

**Documentos Relacionados:**
- `AWS_LAMBDA_CARTOLA_PRD.md` - PRD t√©cnico
- `CONCILIACION_EJECUTIVA_AWS_LAMBDA.md` - Executive summary
- `ARQUITECTURA_COMPARATIVA_GCP_AWS.md` - Comparativa t√©cnica

**Contacto:** dev-team@nubox.com

---

**√öltima Actualizaci√≥n:** 24 de Noviembre, 2025  
**Versi√≥n:** 1.0  
**Estado:** ‚úÖ Completo

