# ğŸ“Š Sistema de EvaluaciÃ³n Masiva - Agentes S1 y M1

**PropÃ³sito:** Evaluar calidad de respuestas de los agentes basado en feedback de Sebastian  
**Fecha:** 2025-10-28  
**Agentes:** S1 (GestiÃ³n Bodegas), M1 (Legal Territorial)

---

## ğŸ“ Archivos en esta Carpeta

### **evaluation_template.csv**
Template con todas las preguntas de Sebastian:
- S1: 67 preguntas de gestiÃ³n de bodegas
- M1: 19 preguntas de normativa urbana/construcciÃ³n
- **Total:** 86 preguntas

**Columnas:**
- `id_evaluacion`: ID Ãºnico (S1-001, M1-001, etc.)
- `agente`: S1 o M1
- `pregunta`: Pregunta exacta
- `respuesta`: Respuesta del agente (llenar despuÃ©s de testing)
- `evaluacion_calidad`: poor | fair | good | excellent
- `tiene_referencias`: true | false
- `basura_detectada`: true | false (si refs contienen "INTRODUCCIÃ“N..." etc.)
- `evaluacion_general`: pass | fail
- `notas`: Observaciones adicionales

---

## ğŸ¯ Criterios de EvaluaciÃ³n

### **Calidad de Respuesta:**

**Excellent (10/10):**
- âœ… Respuesta especÃ­fica y completa
- âœ… Con referencias a documentos
- âœ… Pasos concretos o informaciÃ³n detallada
- âœ… Sin errores evidentes

**Good (7-9/10):**
- âœ… Respuesta relevante
- âœ… InformaciÃ³n Ãºtil pero no completa
- âš ï¸ Puede faltar algÃºn detalle
- âœ… Con o sin referencias

**Fair (4-6/10):**
- âš ï¸ Respuesta genÃ©rica pero relacionada
- âš ï¸ Falta informaciÃ³n especÃ­fica
- âš ï¸ Puede decir "consulta documento X"

**Poor (1-3/10):**
- âŒ No responde la pregunta
- âŒ Dice "no tengo informaciÃ³n"
- âŒ Respuesta irrelevante

---

### **Referencias:**

**Valid:**
- âœ… Badges clickeables [1][2][3]
- âœ… Panel de referencias muestra fragmentos
- âœ… Fragmentos contienen texto Ãºtil

**Garbage:**
- âŒ "1. INTRODUCCIÃ“N .............."
- âŒ "PÃ¡gina X de Y"
- âŒ Solo nÃºmeros/puntos
- âŒ <50 caracteres de contenido

---

### **EvaluaciÃ³n General:**

**PASS:**
- Calidad â‰¥ Good (7/10)
- O calidad Fair (5/10) con referencias vÃ¡lidas

**FAIL:**
- Calidad Poor (<4/10)
- O Fair sin referencias

---

## ğŸ§ª CÃ³mo Usar Este Sistema

### **Paso 1: Testing Manual (Muestra PequeÃ±a)**

**Probar 4-5 preguntas clave primero:**

```bash
# Ejecutar script de testing
npx tsx scripts/test-sebastian-questions.ts
```

Esto probarÃ¡:
- S1: 4 preguntas mÃ¡s reportadas
- M1: 3 preguntas mÃ¡s reportadas

**Output:**
- Respuestas en console
- JSON con resultados en `bulk_evaluation/sebastian_test_results.json`

---

### **Paso 2: EvaluaciÃ³n de Muestra**

Revisar resultados y decidir:

**Si â‰¥70% son Good o Excellent:**
```
âœ… Calidad aceptable
â†’ Proceder con evaluaciÃ³n masiva (86 preguntas)
```

**Si <70% son Good o Excellent:**
```
âŒ Calidad insuficiente
â†’ Investigar causas
â†’ Aplicar mÃ¡s fixes
â†’ Re-testear muestra
```

---

### **Paso 3: EvaluaciÃ³n Masiva (Si Paso 2 fue exitoso)**

**Script automÃ¡tico:**
```bash
# Crear y ejecutar script que:
# 1. Lee evaluation_template.csv
# 2. Hace cada pregunta al agente correspondiente
# 3. EvalÃºa calidad automÃ¡ticamente
# 4. Llena CSV con resultados
# 5. Genera reporte

npx tsx scripts/bulk-evaluate.ts
```

**Tiempo estimado:**
- 86 preguntas Ã— ~5 segundos = ~7 minutos
- + tiempo de evaluaciÃ³n AI

---

### **Paso 4: AnÃ¡lisis de Resultados**

**MÃ©tricas clave:**
```
Total preguntas: 86
Excellent: X (Y%)
Good: X (Y%)
Fair: X (Y%)
Poor: X (Y%)

Con referencias: X (Y%)
Con basura: X (Y%)

Por agente:
- S1: X% calidad aceptable
- M1: X% calidad aceptable
```

**DecisiÃ³n:**
- â‰¥70% aceptable â†’ âœ… Listo para producciÃ³n
- <70% aceptable â†’ âŒ MÃ¡s trabajo necesario

---

## ğŸ“Š Workflow

```
1. Testing Muestra (4-5 preguntas)
   â†“
2. Evaluar Calidad
   â†“
   â‰¥70% Good? 
   â”œâ”€ SÃ â†’ Continuar
   â””â”€ NO â†’ Investigar y arreglar
   â†“
3. Re-indexing (si aplicable)
   â†“
4. Testing Masivo (86 preguntas)
   â†“
5. AnÃ¡lisis Final
   â†“
6. DecisiÃ³n: ProducciÃ³n o MÃ¡s Trabajo
```

---

## ğŸš€ Scripts Disponibles

### **test-sebastian-questions.ts**
- Prueba muestra de preguntas reportadas
- EvalÃºa calidad automÃ¡ticamente
- Genera JSON con resultados

### **bulk-evaluate.ts** (por crear)
- Lee evaluation_template.csv
- Prueba todas las 86 preguntas
- Llena CSV con resultados
- Genera reporte HTML/PDF

### **analyze-results.ts** (por crear)
- Lee CSV completo
- Genera mÃ©tricas
- Identifica patrones de fallos
- Recomienda acciones

---

## ğŸ“ Formato de Reporte

### **CSV Output:**
```csv
id_evaluacion,agente,pregunta,respuesta,evaluacion_calidad,tiene_referencias,basura_detectada,evaluacion_general,notas
S1-001,S1,Â¿CÃ³mo se hace una Solped?,"Para hacer una Solped[1]...",excellent,true,false,pass,Referencias vÃ¡lidas
S1-002,S1,Â¿DÃ³nde busco cÃ³digos?,"Los cÃ³digos se encuentran en...",good,false,false,pass,Sin referencias pero Ãºtil
M1-001,M1,Â¿QuÃ© es un OGUC?,"La OGUC es[1]...",excellent,true,false,pass,5 refs vÃ¡lidas sin basura
```

### **JSON Output:**
```json
{
  "testDate": "2025-10-28",
  "totalQuestions": 86,
  "results": [...],
  "summary": {
    "s1": { "total": 67, "excellent": 45, "good": 15, "fair": 5, "poor": 2 },
    "m1": { "total": 19, "excellent": 15, "good": 3, "fair": 1, "poor": 0 },
    "overall": { "acceptableRate": 0.85 }
  },
  "recommendation": "ready_for_production"
}
```

---

## ğŸ¯ PrÃ³ximos Pasos

### **AHORA:**
1. Ejecutar `npx tsx scripts/test-sebastian-questions.ts`
2. Revisar resultados de muestra (7 preguntas)
3. Decidir si calidad es aceptable

### **Si muestra es buena:**
4. Crear `scripts/bulk-evaluate.ts`
5. Ejecutar evaluaciÃ³n masiva (86 preguntas)
6. Analizar resultados
7. Decidir: ProducciÃ³n o MÃ¡s fixes

### **Si muestra es mala:**
4. Investigar causas de baja calidad
5. Aplicar fixes adicionales
6. Re-probar muestra
7. Repetir hasta calidad aceptable

---

**Listo para comenzar evaluaciÃ³n.** ğŸš€

