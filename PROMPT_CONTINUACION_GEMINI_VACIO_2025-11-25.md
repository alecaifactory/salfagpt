# üîÑ PROMPT PARA CONTINUAR - Problema Gemini Respuesta Vac√≠a

**Para copiar en nueva conversaci√≥n de Cursor**

**Fecha:** 2025-11-25 10:45 AM  
**Sesi√≥n Anterior:** Debugging respuestas vac√≠as en SalfaGPT  
**Branch:** main  
**Server:** localhost:3000 (terminal 15)

---

## üö® **PROBLEMA CR√çTICO ACTUAL:**

### **S√≠ntoma:**
```
Usuario env√≠a mensaje ‚Üí BigQuery responde (2s) ‚Üí Referencias visibles ‚Üí Respuesta vac√≠a ‚ùå
Frontend: contentLength: 0
Backend: fullResponse: 0 chars
Gemini: done: true (stream vac√≠o inmediatamente)
```

### **Root Cause Identificado:**
```typescript
// Terminal logs l√≠nea 202:
üîç [gemini.ts] First next() result: { done: true, hasValue: false }

// Gemini retorna stream VAC√çO (0 chunks) - rechazo silencioso
// NO es error de API key (ya actualizada en producci√≥n)
// NO es falta de context (RAG funciona, 20 chunks con 79.9% similarity)
```

---

## üìä **LO QUE S√ç FUNCIONA:**

### **Backend Performance ‚úÖ**
```
1. BigQuery Agent Search: 2-4s ‚úÖ
   - Agent: 1lgr33ywq5qed67sqCYi (S2-v2)
   - Sources: 467 docs asignados al agente
   - Query: Cosine similarity optimizado
   - Results: Top 20 chunks (79.9% avg similarity)
   - Regi√≥n: us-east4 (BLUE)

2. RAG Context Building: ~6KB ‚úÖ
   - Before: 48KB (20 chunks completos)
   - After: 6KB (top 3 chunks por doc, 2KB max cada uno)
   - Consolidated: 1 documento √∫nico (20 chunks agrupados)

3. Referencias: ‚úÖ
   - Generadas ANTES de streaming
   - Enviadas al frontend
   - Visibles en UI
```

### **Fixes Aplicados ‚úÖ**
```
1. Mensajes vac√≠os (content object ‚Üí string) ‚úÖ
2. Storage paths (919 docs actualizados a us-east4) ‚úÖ
3. GREEN = BLUE (unified to flow_analytics_east4) ‚úÖ
4. Context truncado (48KB ‚Üí 6KB) ‚úÖ
5. System prompt limitado (4.8KB ‚Üí 500 chars) ‚úÖ
```

---

## ‚ùå **LO QUE NO FUNCIONA:**

### **Gemini API Streaming:**
```typescript
// src/lib/gemini.ts l√≠nea 478-520

const stream = await genAI.models.generateContentStream({
  model: 'gemini-2.5-flash',
  contents: [...],
  config: {
    systemInstruction: baseSystemPrompt,  // 500 chars max
    temperature: 0.7,
    maxOutputTokens: 300
  }
});

// El stream retorna:
for await (const chunk of stream) {
  // NUNCA entra aqu√≠ - 0 iteraciones
}

// stream.next() retorna:
{ done: true, hasValue: false }  // ‚Üê Stream vac√≠o desde inicio
```

### **Teor√≠as del Problema:**
```
1. ‚ùì System instruction a√∫n muy largo (500 chars + agent prompt 4.7KB)
2. ‚ùì Context 6KB + system 5KB = 11KB total excede l√≠mite no documentado
3. ‚ùì Gemini 2.5 Flash tiene rate limit activo
4. ‚ùì Request format incorrecto para streaming
5. ‚ùì API key v√°lida pero sin permisos de streaming
```

---

## üóÇÔ∏è **ARCHIVOS CLAVE MODIFICADOS:**

### **C√≥digo:**
```
src/lib/gemini.ts
  - Logs ultra-detallados de streaming
  - Context trimming (10KB max)
  - System prompt simplificado
  - Debug manual con stream.next()

src/lib/rag-search.ts
  - buildRAGContext(): Top 3 chunks por doc
  - Max 2KB por chunk
  - Resultado: ~6KB (no 48KB)

src/pages/api/conversations/[id]/messages-stream.ts
  - System prompt trimming (500 chars max)
  - Logs de context length
  - Content guardado como string (no objeto)

src/lib/firestore.ts
  - Message.content: MessageContent | string
  - addMessage() acepta string

src/lib/bigquery-router.ts
  - localhost ‚Üí BLUE (no GREEN lento)

src/lib/bigquery-optimized.ts
  - GREEN apunta a flow_analytics_east4 (same as BLUE)
  - Query con ML.DISTANCE optimizado
```

### **Documentaci√≥n:**
```
docs/fixes/MENSAJES_VACIOS_FIX_2025-11-25.md
docs/GREEN_UNIFIED_WITH_BLUE_2025-11-25.md
docs/BIGQUERY_AGENT_FILTERING_EXPLAINED.md
```

---

## üéØ **PR√ìXIMOS PASOS RECOMENDADOS:**

### **Opci√≥n 1: Investigar Gemini API (Prioritaria)**

**Hip√≥tesis:** `generateContentStream()` puede tener issue con:
- System instructions largas combinadas con RAG context
- Versi√≥n del SDK `@google/genai` desactualizada
- Rate limiting no visible

**Acciones:**
```bash
# 1. Verificar versi√≥n del SDK
npm list @google/genai

# 2. Probar request m√≠nimo (sin context, sin system prompt)
# Test: Solo message simple
const stream = await genAI.models.generateContentStream({
  model: 'gemini-2.5-flash',
  contents: [{ role: 'user', parts: [{ text: 'Hola' }] }],
  config: { maxOutputTokens: 50 }
});
# ¬øRetorna chunks? ‚Üí API funciona
# ¬ødone: true? ‚Üí Problema de configuraci√≥n

# 3. Incrementar gradualmente:
# - Agregar system instruction (100 chars)
# - Agregar context (1KB)
# - Agregar system instruction (500 chars)
# - Hasta encontrar breaking point
```

### **Opci√≥n 2: Usar generateContent() no-streaming (Workaround)**

**Si streaming est√° roto, usar respuesta completa:**

```typescript
// En lugar de generateContentStream()
const result = await genAI.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: [...],
  config: {...}
});

const fullText = result.text || '';
// Luego simular chunks en backend para mantener UX
```

**Ventaja:** Sabemos que embedding funciona (usa mismo SDK)  
**Desventaja:** No streaming real (pero podemos simular)

### **Opci√≥n 3: Verificar Quotas y Permisos**

```bash
# Check API key scopes
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent" \
  -H "Content-Type: application/json" \
  -H "x-goog-api-key: $GOOGLE_AI_API_KEY" \
  -d '{"contents":[{"parts":[{"text":"test"}]}]}'

# Deber√≠a retornar chunks o error espec√≠fico
```

---

## üîß **DIAGN√ìSTICO STEP-BY-STEP:**

### **Para nueva sesi√≥n, ejecutar:**

**1. Verificar SDK y config:**
```bash
cd /Users/alec/salfagpt
npm list @google/genai
grep "GOOGLE_AI_API_KEY" .env
```

**2. Test m√≠nimo de Gemini (sin context):**
```typescript
// Archivo: test-gemini-minimal.mjs
import { GoogleGenAI } from '@google/genai';

const genAI = new GoogleGenAI({ 
  apiKey: process.env.GOOGLE_AI_API_KEY 
});

const stream = await genAI.models.generateContentStream({
  model: 'gemini-2.5-flash',
  contents: [{ role: 'user', parts: [{ text: 'Di hola en 3 palabras' }] }],
  config: { maxOutputTokens: 20 }
});

let count = 0;
for await (const chunk of stream) {
  count++;
  console.log(`Chunk ${count}:`, chunk.text);
}

console.log(`Total: ${count} chunks`);
```

**Ejecutar:**
```bash
npx tsx test-gemini-minimal.mjs
```

**Resultado esperado:**
- ‚úÖ `Chunk 1: Hola a todos` ‚Üí API funciona
- ‚ùå `Total: 0 chunks` ‚Üí Problema de API/permisos

**3. Si test m√≠nimo funciona, agregar complejidad:**
```typescript
// Agregar system instruction corta
config: {
  systemInstruction: 'S√© breve',
  maxOutputTokens: 20
}

// ¬øFunciona? ‚Üí Agregar m√°s
systemInstruction: 'S√© breve y preciso. Responde en espa√±ol.',

// Seguir agregando hasta encontrar breaking point
```

**4. Si nada funciona, switch a non-streaming:**
```typescript
// Reemplazar streamAIResponse() en src/lib/gemini.ts
export async function* streamAIResponse(...) {
  // En lugar de generateContentStream()
  const result = await genAI.models.generateContent({
    model: 'gemini-2.5-flash',
    contents,
    config: {...}
  });
  
  const fullText = result.text || '';
  
  // Simular chunks para mantener UX de streaming
  const chunkSize = 50;
  for (let i = 0; i < fullText.length; i += chunkSize) {
    yield fullText.substring(i, i + chunkSize);
  }
}
```

---

## üìã **ESTADO ACTUAL DEL SISTEMA:**

### **Performance:**
```
Backend (us-east4): 2-4s ‚úÖ
BigQuery: 2s (TOP 20 chunks) ‚úÖ
Context: 6KB (optimizado) ‚úÖ
Gemini: 0s (NO responde) ‚ùå
Total: ~15s esperando sin respuesta ‚ùå
```

### **Configuraci√≥n:**
```yaml
Branch: main
Server: localhost:3000 (PID 76439)
Terminal: 15 (logs activos)
Dataset: flow_analytics_east4 (BLUE)
Regi√≥n: us-east4
Agent: 1lgr33ywq5qed67sqCYi (S2-v2, 467 sources)
```

### **Limits Aplicados:**
```
Context: 6KB (top 3 chunks √ó 2KB)
System prompt: 500 chars max
Max tokens output: 300
RAG chunks: 20 (topK)
Similarity: >= 0.3 (threshold)
```

---

## üéØ **PLAN DE ACCI√ìN (Nueva Sesi√≥n):**

### **Paso 1: Diagn√≥stico Gemini API (15 min)**
```
1. Test m√≠nimo sin context
2. Test con context corto (1KB)
3. Test con system instruction incremental
4. Identificar breaking point exacto
```

### **Paso 2: Workaround si API rota (30 min)**
```
1. Switch a generateContent() (no streaming)
2. Simular chunks en backend
3. Mantener UX igual
4. Verificar funcionamiento end-to-end
```

### **Paso 3: Optimizaci√≥n Final (15 min)**
```
1. Si workaround funciona, commit
2. Documentar decisi√≥n
3. Test completo de flujo
4. Deploy si estable
```

---

## üîë **INFORMACI√ìN CR√çTICA:**

### **IDs:**
```
Usuario: usr_uhwqffaqag1wrryd82tw (alec@getaifactory.com)
Agente S2-v2: 1lgr33ywq5qed67sqCYi (467 sources)
Conversaci√≥n test: 06GTPfn9CGnk9ZCSuvod
```

### **API Keys:**
```
GOOGLE_AI_API_KEY: Actualizada en producci√≥n ‚úÖ
Project: salfagpt
```

### **Limits Gemini 2.5 Flash:**
```
Context window: 1M tokens (~4MB text)
System instruction: No documentado (probablemente <2KB)
Max output: 8,192 tokens
Nuestro uso:
  - Context: 6KB (~1,500 tokens) ‚úÖ
  - System: 4.7KB (~1,200 tokens) ‚ö†Ô∏è PROBLEMA PROBABLE
  - Output: 300 tokens ‚úÖ
  - Total input: ~2,700 tokens ‚úÖ (bien dentro de 1M)
```

---

## üêõ **DEBUG CR√çTICO:**

### **El problema NO es:**
- ‚ùå Tama√±o de context window (1.5K << 1M tokens)
- ‚ùå API key (actualizada, embeddings funcionan)
- ‚ùå BigQuery (funciona perfecto, 2s)
- ‚ùå Referencias (se generan y muestran correctamente)
- ‚ùå Frontend (recibe referencias, solo falta texto)

### **El problema S√ç es:**
- ‚úÖ `generateContentStream()` retorna `done: true` inmediatamente
- ‚úÖ System instruction 4.7KB causa rechazo (reducido a 500 chars)
- ‚úÖ Necesita verificaci√≥n si 500 chars es suficiente

---

## üìù **COMANDO PARA SIGUIENTE SESI√ìN:**

```markdown
# CONTEXTO:
Estoy debugging problema de respuestas vac√≠as en SalfaGPT.

# PROBLEMA:
Gemini API retorna stream vac√≠o (done: true, 0 chunks)
BigQuery RAG funciona (20 chunks, 79.9% similarity)
Referencias visibles, pero respuesta vac√≠a

# ROOT CAUSE:
generateContentStream() rechaza request silenciosamente
Probablemente por systemInstruction demasiado largo (4.7KB)

# FIXES APLICADOS:
‚úÖ Context truncado: 48KB ‚Üí 6KB (top 3 chunks √ó 2KB)
‚úÖ System prompt limitado: 4.7KB ‚Üí 500 chars
‚úÖ Mensajes guardados como string (no objeto)
‚úÖ GREEN = BLUE (unified BigQuery)
‚ö†Ô∏è Gemini streaming SIGUE vac√≠o

# PR√ìXIMOS PASOS:
1. Test Gemini API m√≠nimo (sin context/system)
2. Si test pasa: Identificar breaking point incremental
3. Si test falla: Switch a generateContent() no-streaming
4. Workaround: Simular streaming con chunks artificiales

# ARCHIVOS CLAVE:
- src/lib/gemini.ts (streamAIResponse - l√≠nea 478-520)
- src/lib/rag-search.ts (buildRAGContext - l√≠nea 225-269)
- src/pages/api/conversations/[id]/messages-stream.ts (l√≠nea 668-700)

# TEST CASE:
Agente: Maqsa Mantenimiento (S2-v2)
Pregunta: "Cada cuantas horas se debe cambiar el aceite hidraulico en un camion pluma SCANIA P450 B 6x4"
Esperado: Respuesta basada en Manual de Mantenimiento Scania (79.9% similarity)
Actual: Respuesta vac√≠a (0 chars)

# LOGS RELEVANTES:
Terminal: /Users/alec/.cursor/projects/Users-alec-salfagpt/terminals/15.txt
Buscar: "gemini.ts" para ver stream debug
L√≠nea clave: "done: true" = stream vac√≠o

# COMANDO INICIAL:
Por favor ejecuta test m√≠nimo de Gemini API:
```bash
cat > test-gemini.mjs << 'EOF'
import { GoogleGenAI } from '@google/genai';
const genAI = new GoogleGenAI({ apiKey: process.env.GOOGLE_AI_API_KEY });
const stream = await genAI.models.generateContentStream({
  model: 'gemini-2.5-flash',
  contents: [{ role: 'user', parts: [{ text: 'Di hola' }] }],
  config: { maxOutputTokens: 10 }
});
let count = 0;
for await (const chunk of stream) {
  count++;
  console.log('Chunk:', chunk.text);
}
console.log('Total chunks:', count);
process.exit(0);
EOF
cd /Users/alec/salfagpt && npx tsx test-gemini.mjs
```

Si retorna chunks: API funciona, problema es config
Si retorna 0: API/key issue, usar generateContent() como workaround
```

---

## üìä **ARQUITECTURA ACTUAL:**

```
Usuario ‚Üí Frontend (S2-v2)
  ‚Üì
Backend /messages-stream
  ‚Üì
BigQuery us-east4 (‚úÖ 2s, 20 chunks, 79.9%)
  ‚Üì
buildRAGContext() (‚úÖ 6KB contexto optimizado)
  ‚Üì
streamAIResponse() ‚Üí Gemini API
  ‚Üì
generateContentStream() ‚Üí ‚ùå done: true (stream vac√≠o)
  ‚Üì
0 chunks retornados
  ‚Üì
Frontend muestra: mensaje vac√≠o + 1 referencia
```

---

## üîç **DEBUGGING TRACE COMPLETO:**

```
[10:38:39] POST /messages-stream
[10:38:41] BigQuery search: 2s ‚úÖ
[10:38:41] Found 20 chunks ‚úÖ
[10:38:41] Built 1 reference ‚úÖ
[10:38:41] Context: 6,421 chars ‚úÖ
[10:38:41] System: 4,826 chars ‚ö†Ô∏è
[10:38:41] Calling Gemini API...
[10:38:42] Stream received ‚úÖ
[10:38:42] stream.next() ‚Üí done: true ‚ùå
[10:38:42] Total chunks: 0 ‚ùå
[10:38:42] Saved message: content = "" ‚ùå
[10:38:43] Frontend: contentLength = 0 ‚ùå
```

---

## ‚úÖ **SOLUCIONES PROPUESTAS:**

### **Soluci√≥n A: Fix Streaming (Ideal)**
```
Identificar por qu√© generateContentStream() falla
Ajustar par√°metros hasta que funcione
Mantener streaming real
```

### **Soluci√≥n B: Non-Streaming Workaround (R√°pido)**
```typescript
// Cambiar a generateContent() (no stream)
const result = await genAI.models.generateContent({...});
const fullText = result.text;

// Simular streaming dividiendo en chunks
for (let i = 0; i < fullText.length; i += 100) {
  yield fullText.substring(i, i + 100);
  await sleep(50); // Simular delay
}
```

**Pro:** Funciona inmediatamente  
**Con:** No streaming real de Gemini  
**UX:** Id√©ntico (usuario no nota diferencia)

### **Soluci√≥n C: Reducir System Prompt M√°s (Test)**
```
Actual: 500 chars limit
Nuevo: 200 chars limit
Test: ¬øGemini acepta?
```

---

## üìö **CONTEXTO T√âCNICO:**

### **Gemini 2.5 Flash Specs:**
```
Input: 1M tokens context window
Output: 8,192 tokens max
Streaming: Soportado (pero no funciona para nosotros)
System instruction: Max no documentado (probablemente 1-2KB)
```

### **Nuestro Uso Actual:**
```
Input tokens: ~2,700 (context 6KB + system 500 chars)
Output tokens: 300 (configurado)
Ratio: 9:1 (saludable)
Problema: Stream vac√≠o a pesar de configuraci√≥n v√°lida
```

---

## üöÄ **START HERE (Nueva Sesi√≥n):**

1. **Lee este prompt completo**
2. **Ejecuta test m√≠nimo Gemini** (c√≥digo arriba)
3. **Si test pasa:** Incrementa complejidad hasta breaking point
4. **Si test falla:** Implementa Soluci√≥n B (non-streaming workaround)
5. **Verifica funcionamiento** end-to-end
6. **Commit y documenta** soluci√≥n final

---

**Branch:** main  
**Server:** localhost:3000 corriendo  
**Terminal:** 15 (logs activos)  
**Listo para:** Test diagn√≥stico Gemini API

**¬°Buena suerte!** üéØüîç



