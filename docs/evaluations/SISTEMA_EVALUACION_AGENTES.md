# ğŸ¯ Sistema de EvaluaciÃ³n de Calidad de Agentes

**VersiÃ³n:** 1.0.0  
**Fecha:** 2025-10-29  
**PropÃ³sito:** Framework completo para evaluar, registrar y mejorar la calidad de respuestas de agentes S001 y M001

---

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SISTEMA DE EVALUACIÃ“N DE CALIDAD                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. BANCO DE PREGUNTAS                                      â”‚
â”‚     â””â”€ questions/                                           â”‚
â”‚        â”œâ”€ S001-questions-v1.json (66 preguntas)            â”‚
â”‚        â””â”€ M001-questions-v1.json (19 preguntas)            â”‚
â”‚                                                             â”‚
â”‚  2. EVALUACIONES                                            â”‚
â”‚     â””â”€ evaluations/                                         â”‚
â”‚        â”œâ”€ EVAL-S001-2025-10-29-v1/                         â”‚
â”‚        â”‚  â”œâ”€ metadata.json                                  â”‚
â”‚        â”‚  â”œâ”€ responses/                                     â”‚
â”‚        â”‚  â”‚  â”œâ”€ Q001-response.json                         â”‚
â”‚        â”‚  â”‚  â”œâ”€ Q001-response.md (readable)                â”‚
â”‚        â”‚  â”‚  â””â”€ ... (66 archivos)                          â”‚
â”‚        â”‚  â”œâ”€ references/                                    â”‚
â”‚        â”‚  â”‚  â”œâ”€ Q001-references.json                       â”‚
â”‚        â”‚  â”‚  â””â”€ ... (66 archivos)                          â”‚
â”‚        â”‚  â”œâ”€ expert-feedback/                               â”‚
â”‚        â”‚  â”‚  â”œâ”€ Q001-feedback.json                         â”‚
â”‚        â”‚  â”‚  â””â”€ ... (66 archivos)                          â”‚
â”‚        â”‚  â””â”€ summary-report.md                             â”‚
â”‚        â”‚                                                     â”‚
â”‚        â””â”€ EVAL-M001-2025-10-29-v1/                         â”‚
â”‚           â””â”€ ... (estructura similar)                       â”‚
â”‚                                                             â”‚
â”‚  3. ITERACIONES                                             â”‚
â”‚     â””â”€ iterations/                                          â”‚
â”‚        â”œâ”€ S001-v1-to-v2-improvements.md                    â”‚
â”‚        â””â”€ M001-v1-to-v2-improvements.md                    â”‚
â”‚                                                             â”‚
â”‚  4. REPORTES COMPARATIVOS                                   â”‚
â”‚     â””â”€ reports/                                             â”‚
â”‚        â”œâ”€ S001-quality-trend.md                            â”‚
â”‚        â”œâ”€ M001-quality-trend.md                            â”‚
â”‚        â””â”€ comparative-analysis.md                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Estructura de Datos

### **1. Pregunta (Question)**

```json
{
  "id": "S001-Q001",
  "agent": "S001",
  "version": "v1",
  "category": "CÃ³digos y CatÃ¡logos",
  "question": "Â¿DÃ³nde busco los cÃ³digos de materiales?",
  "priority": "critical",
  "expectedTopics": [
    "SAP",
    "cÃ³digo de material",
    "transacciÃ³n",
    "catÃ¡logo"
  ],
  "createdAt": "2025-10-29T12:00:00Z",
  "createdBy": "Alec"
}
```

---

### **2. Respuesta (Response)**

```json
{
  "id": "S001-Q001-R001",
  "questionId": "S001-Q001",
  "evaluationId": "EVAL-S001-2025-10-29-v1",
  "timestamp": "2025-10-29T12:30:00Z",
  "model": "gemini-2.5-flash",
  
  "response": {
    "text": "Para buscar cÃ³digos de materiales en SAP...",
    "markdown": "# Respuesta\n\nPara buscar cÃ³digos...",
    "lengthChars": 1245,
    "lengthTokens": 312
  },
  
  "references": {
    "count": 3,
    "items": [
      {
        "refId": 1,
        "sourceId": "abc123",
        "documentName": "PP-009 Como Imprimir...",
        "similarity": 81.5,
        "chunkCount": 2,
        "chunkIds": ["chunk1", "chunk7"],
        "fragmentNumber": 0,
        "tokensUsed": 3468,
        "preview": "MAQSA Abastecimiento..."
      }
    ]
  },
  
  "ragMetrics": {
    "searchTimeMs": 450,
    "topKChunks": 10,
    "consolidatedDocs": 3,
    "totalTokensContext": 12450,
    "contextWindowUsed": 0.12
  },
  
  "validation": {
    "phantomRefsDetected": false,
    "allRefsInRange": true,
    "maxRefUsed": 3,
    "totalBadges": 3,
    "consistent": true
  }
}
```

---

### **3. Feedback de Experto (Expert Feedback)**

```json
{
  "id": "S001-Q001-F001",
  "responseId": "S001-Q001-R001",
  "expertId": "sebastian@salfa.cl",
  "expertName": "Sebastian",
  "expertRole": "Especialista GestiÃ³n Bodegas",
  "timestamp": "2025-10-29T14:00:00Z",
  
  "evaluation": {
    "qualityScore": 9,
    "scoreBreakdown": {
      "accuracy": 10,
      "completeness": 9,
      "usefulness": 9,
      "clarity": 8
    },
    "comments": "Respuesta muy completa. Menciona transacciÃ³n correcta y pasos claros.",
    "approved": true
  },
  
  "contentValidation": {
    "technicallyCorrect": true,
    "referencesRelevant": true,
    "missingInformation": false,
    "incorrectInformation": false,
    "suggestions": [
      "PodrÃ­a agregar screenshot de la transacciÃ³n SAP"
    ]
  },
  
  "referencesValidation": {
    "allRelevant": true,
    "bestReference": 1,
    "unusedButRelevant": [],
    "irrelevantShown": [],
    "missingDocuments": []
  }
}
```

---

### **4. Resumen de EvaluaciÃ³n (Evaluation Summary)**

```json
{
  "id": "EVAL-S001-2025-10-29-v1",
  "agent": "S001",
  "version": "v1",
  "date": "2025-10-29",
  "evaluator": "Alec + Sebastian",
  
  "scope": {
    "totalQuestions": 66,
    "questionsTested": 10,
    "coverage": 15.2
  },
  
  "results": {
    "averageQuality": 9.1,
    "qualityDistribution": {
      "excellent_9_10": 8,
      "good_7_8": 2,
      "acceptable_5_6": 0,
      "poor_below_5": 0
    },
    "phantomRefsDetected": 0,
    "technicalIssues": 0
  },
  
  "byCategory": [
    {
      "category": "GestiÃ³n Combustible",
      "questionsTested": 1,
      "averageQuality": 10.0,
      "phantomRefs": 0
    }
  ],
  
  "expertFeedback": {
    "totalResponses": 10,
    "approved": 9,
    "needsImprovement": 1,
    "rejected": 0,
    "averageScore": 9.1
  },
  
  "improvements": [
    {
      "priority": "high",
      "issue": "Algunos fragmentos con contenido limitado",
      "affectedQuestions": ["Q004", "Q012"],
      "proposedSolution": "Re-extracciÃ³n de PDFs con mejor configuraciÃ³n"
    }
  ]
}
```

---

## ğŸ“‚ Estructura de Directorios

```bash
docs/evaluations/
â”œâ”€â”€ README.md                          # Este archivo
â”‚
â”œâ”€â”€ questions/                         # Banco de preguntas
â”‚   â”œâ”€â”€ S001-questions-v1.json        # 66 preguntas S001
â”‚   â”œâ”€â”€ M001-questions-v1.json        # 19 preguntas M001
â”‚   â””â”€â”€ template-question.json        # Template para nuevas
â”‚
â”œâ”€â”€ evaluations/                       # Evaluaciones realizadas
â”‚   â”œâ”€â”€ EVAL-S001-2025-10-29-v1/
â”‚   â”‚   â”œâ”€â”€ metadata.json             # Info de la evaluaciÃ³n
â”‚   â”‚   â”œâ”€â”€ responses/
â”‚   â”‚   â”‚   â”œâ”€â”€ Q001-response.json    # Respuesta estructurada
â”‚   â”‚   â”‚   â”œâ”€â”€ Q001-response.md      # Respuesta legible
â”‚   â”‚   â”‚   â””â”€â”€ ... (66 archivos)
â”‚   â”‚   â”œâ”€â”€ references/
â”‚   â”‚   â”‚   â”œâ”€â”€ Q001-references.json  # Referencias obtenidas
â”‚   â”‚   â”‚   â””â”€â”€ ... (66 archivos)
â”‚   â”‚   â”œâ”€â”€ expert-feedback/
â”‚   â”‚   â”‚   â”œâ”€â”€ Q001-feedback.json    # Feedback de experto
â”‚   â”‚   â”‚   â””â”€â”€ ... (66 archivos)
â”‚   â”‚   â””â”€â”€ summary-report.md         # Reporte consolidado
â”‚   â”‚
â”‚   â””â”€â”€ EVAL-M001-2025-10-29-v1/
â”‚       â””â”€â”€ ... (estructura similar)
â”‚
â”œâ”€â”€ iterations/                        # Mejoras entre versiones
â”‚   â”œâ”€â”€ S001-v1-to-v2.md
â”‚   â””â”€â”€ M001-v1-to-v2.md
â”‚
â””â”€â”€ reports/                           # Reportes comparativos
    â”œâ”€â”€ quality-trend-S001.md
    â”œâ”€â”€ quality-trend-M001.md
    â””â”€â”€ comparative-S001-vs-M001.md
```

---

## ğŸ”„ Workflow de EvaluaciÃ³n

### **Paso 1: PreparaciÃ³n**

```bash
# Crear nueva evaluaciÃ³n
mkdir -p docs/evaluations/EVAL-S001-$(date +%Y-%m-%d)-v1/{responses,references,expert-feedback}

# Copiar preguntas
cp docs/evaluations/questions/S001-questions-v1.json \
   docs/evaluations/EVAL-S001-$(date +%Y-%m-%d)-v1/questions.json
```

---

### **Paso 2: EjecuciÃ³n de Tests**

Para cada pregunta:

```javascript
// 1. Enviar pregunta al agente
const response = await sendToAgent(question);

// 2. Capturar respuesta completa
const responseData = {
  text: response.text,
  references: response.references,
  ragMetrics: response.metrics,
  timestamp: new Date()
};

// 3. Validar tÃ©cnicamente
const validation = {
  phantomRefs: checkPhantomRefs(response),
  refsInRange: checkRefsInRange(response),
  consistent: checkConsistency(response)
};

// 4. Guardar
await saveResponse(questionId, responseData, validation);
```

---

### **Paso 3: EvaluaciÃ³n de Expertos**

**Template de EvaluaciÃ³n:**

```markdown
# EvaluaciÃ³n: S001-Q001

## Pregunta
Â¿DÃ³nde busco los cÃ³digos de materiales?

## Respuesta del Agente
[Texto completo de la respuesta]

## Referencias Utilizadas
1. [1] PP-009 ... (81.5% similitud)
2. [2] I-006 ... (79.2% similitud)
3. [3] PP-007 ... (76.8% similitud)

---

## EvaluaciÃ³n del Experto

**Evaluador:** Sebastian  
**Fecha:** 2025-10-29  
**Rol:** Especialista GestiÃ³n Bodegas

### CalificaciÃ³n Global: __/10

### Desglose:
- **PrecisiÃ³n TÃ©cnica:** __/10
  - Â¿La informaciÃ³n es correcta segÃºn tu conocimiento?
  - Â¿Menciona transacciones/procedimientos correctos?

- **Completitud:** __/10
  - Â¿Responde completamente la pregunta?
  - Â¿Falta informaciÃ³n importante?

- **Utilidad:** __/10
  - Â¿Puedes usar esta respuesta en tu trabajo?
  - Â¿Es accionable?

- **Claridad:** __/10
  - Â¿Es fÃ¡cil de entender?
  - Â¿EstÃ¡ bien estructurada?

### Referencias:
- Â¿Son relevantes las referencias mostradas? â˜ SÃ­ â˜ No â˜ Parcial
- Â¿La referencia mÃ¡s Ãºtil es: â˜ [1] â˜ [2] â˜ [3]
- Â¿Falta algÃºn documento importante? â˜ SÃ­ â˜ No
  - Si sÃ­, Â¿cuÃ¡l?: ___________________

### ValidaciÃ³n de Contenido:
- Â¿La respuesta es tÃ©cnicamente correcta? â˜ SÃ­ â˜ No â˜ Parcial
- Â¿Hay informaciÃ³n incorrecta? â˜ SÃ­ â˜ No
  - Si sÃ­, Â¿quÃ©?: ___________________
  
### AprobaciÃ³n:
- â˜ **Aprobada** - Puede usarse en producciÃ³n
- â˜ **Necesita Mejora** - Funcional pero mejorable
- â˜ **Rechazada** - No es Ãºtil o incorrecta

### Comentarios:
[Espacio para observaciones detalladas]

### Sugerencias de Mejora:
[QuÃ© deberÃ­a agregarse/modificarse/eliminarse]
```

---

### **Paso 4: ConsolidaciÃ³n**

```javascript
// Generar reporte automÃ¡tico
const summary = {
  totalQuestions: 66,
  evaluated: 66,
  averageQuality: calculateAverage(),
  approvalRate: calculateApprovalRate(),
  byCategory: groupByCategory(),
  improvements: extractImprovements(),
  iteration: "v1"
};

await generateSummaryReport(summary);
```

---

## ğŸ“Š Schema de Base de Datos (Firestore)

### **Collection: agent_evaluations**

```typescript
interface AgentEvaluation {
  id: string;                          // EVAL-S001-2025-10-29-v1
  agentId: string;                     // S001 o M001
  agentName: string;                   // GESTION BODEGAS GPT
  version: string;                     // v1, v2, v3...
  date: Date;                          // Fecha de evaluaciÃ³n
  
  scope: {
    totalQuestions: number;            // 66
    questionsTested: number;           // 10 o 66
    coverage: number;                  // % (15% o 100%)
  };
  
  results: {
    averageQuality: number;            // 9.1
    qualityDistribution: {
      excellent_9_10: number;          // 8
      good_7_8: number;                // 2
      acceptable_5_6: number;          // 0
      poor_below_5: number;            // 0
    };
    phantomRefsDetected: number;       // 0
    technicalIssues: number;           // 0
  };
  
  expertValidation: {
    evaluators: string[];              // ["sebastian@salfa.cl"]
    totalResponses: number;            // 10
    approved: number;                  // 9
    needsImprovement: number;          // 1
    rejected: number;                  // 0
    averageScore: number;              // 9.1
  };
  
  status: 'draft' | 'in-review' | 'completed' | 'archived';
  createdBy: string;
  createdAt: Date;
  updatedAt: Date;
}
```

---

### **Collection: evaluation_responses**

```typescript
interface EvaluationResponse {
  id: string;                          // S001-Q001-R001
  evaluationId: string;                // EVAL-S001-2025-10-29-v1
  questionId: string;                  // S001-Q001
  agentId: string;                     // S001
  
  question: {
    text: string;
    category: string;
    priority: string;
  };
  
  response: {
    text: string;                      // Texto completo
    markdown: string;                  // Formato markdown
    lengthChars: number;
    lengthTokens: number;
    generatedAt: Date;
  };
  
  references: {
    count: number;                     // 3
    items: Array<{
      refId: number;                   // 1, 2, 3
      sourceId: string;
      documentName: string;
      similarity: number;              // 81.5%
      chunkCount: number;
      chunkIds: string[];
      fragmentNumber: number;
      tokensUsed: number;
      preview: string;
    }>;
  };
  
  ragMetrics: {
    searchTimeMs: number;
    topKChunks: number;
    consolidatedDocs: number;
    totalTokensContext: number;
    contextWindowUsed: number;
  };
  
  validation: {
    phantomRefsDetected: boolean;
    allRefsInRange: boolean;
    maxRefUsed: number;
    totalBadges: number;
    consistent: boolean;
    technicalIssues: string[];
  };
  
  timestamp: Date;
}
```

---

### **Collection: expert_feedback**

```typescript
interface ExpertFeedback {
  id: string;                          // S001-Q001-F001
  responseId: string;                  // S001-Q001-R001
  evaluationId: string;                // EVAL-S001-2025-10-29-v1
  
  expert: {
    id: string;
    email: string;
    name: string;
    role: string;
    department: string;
  };
  
  scores: {
    overall: number;                   // 1-10
    accuracy: number;                  // 1-10
    completeness: number;              // 1-10
    usefulness: number;                // 1-10
    clarity: number;                   // 1-10
  };
  
  validation: {
    technicallyCorrect: boolean;
    referencesRelevant: boolean;
    missingInformation: boolean;
    incorrectInformation: boolean;
  };
  
  references: {
    allRelevant: boolean;
    bestReferenceId: number;           // 1, 2, o 3
    irrelevantShown: number[];         // []
    missingDocuments: string[];        // []
  };
  
  approval: 'approved' | 'needs_improvement' | 'rejected';
  
  comments: string;
  suggestions: string[];
  
  timestamp: Date;
}
```

---

## ğŸ”§ Scripts de AutomatizaciÃ³n

### **Script 1: Generar EvaluaciÃ³n**

```bash
#!/bin/bash
# scripts/generate-evaluation.sh

AGENT=$1  # S001 o M001
VERSION=${2:-v1}
DATE=$(date +%Y-%m-%d)

EVAL_ID="EVAL-${AGENT}-${DATE}-${VERSION}"
EVAL_DIR="docs/evaluations/evaluations/${EVAL_ID}"

echo "ğŸ¯ Generando evaluaciÃ³n: ${EVAL_ID}"

# Crear estructura
mkdir -p ${EVAL_DIR}/{responses,references,expert-feedback}

# Copiar preguntas
cp docs/evaluations/questions/${AGENT}-questions-${VERSION}.json \
   ${EVAL_DIR}/questions.json

# Crear metadata
cat > ${EVAL_DIR}/metadata.json << EOF
{
  "id": "${EVAL_ID}",
  "agent": "${AGENT}",
  "version": "${VERSION}",
  "date": "${DATE}",
  "status": "draft",
  "createdAt": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "createdBy": "$(git config user.email)"
}
EOF

echo "âœ… EvaluaciÃ³n creada en: ${EVAL_DIR}"
```

---

### **Script 2: Ejecutar Testing Automatizado**

```typescript
// scripts/run-evaluation.ts
import { sendToAgent, captureResponse } from './lib/testing';
import { saveResponse, validateResponse } from './lib/storage';

async function runEvaluation(evaluationId: string, questions: Question[]) {
  const results = [];
  
  for (const question of questions) {
    console.log(`\nğŸ§ª Testing: ${question.id} - ${question.question}`);
    
    // 1. Enviar a agente
    const response = await sendToAgent(question.agent, question.question);
    
    // 2. Capturar respuesta completa
    const responseData = await captureResponse(response);
    
    // 3. Validar tÃ©cnicamente
    const validation = validateResponse(responseData);
    
    // 4. Guardar
    await saveResponse(evaluationId, question.id, {
      ...responseData,
      validation
    });
    
    // 5. Mostrar resultado
    console.log(`  âœ… Quality: ${validation.consistent ? 'PASS' : 'FAIL'}`);
    console.log(`  ğŸ“š References: ${responseData.references.count}`);
    console.log(`  âš ï¸ Phantom refs: ${validation.phantomRefsDetected ? 'YES' : 'NO'}`);
    
    results.push({
      questionId: question.id,
      quality: validation.consistent ? 'PASS' : 'FAIL',
      references: responseData.references.count,
      phantomRefs: validation.phantomRefsDetected
    });
  }
  
  // 6. Generar reporte preliminar
  await generatePreliminaryReport(evaluationId, results);
  
  return results;
}
```

---

### **Script 3: Generar Reporte Final**

```typescript
// scripts/generate-report.ts
import { loadEvaluation, loadFeedback } from './lib/storage';

async function generateFinalReport(evaluationId: string) {
  const evaluation = await loadEvaluation(evaluationId);
  const responses = await loadResponses(evaluationId);
  const feedback = await loadFeedback(evaluationId);
  
  const report = {
    evaluationId,
    agent: evaluation.agent,
    date: evaluation.date,
    
    technical: analyzeTechnical(responses),
    expert: analyzeExpert(feedback),
    combined: combineAnalysis(responses, feedback),
    
    improvements: identifyImprovements(responses, feedback),
    nextSteps: generateNextSteps(evaluation)
  };
  
  await saveReport(evaluationId, report);
  
  return report;
}
```

---

## ğŸ“‹ Templates

### **Template: Question JSON**

```json
{
  "id": "S001-Q001",
  "agent": "S001",
  "version": "v1",
  "category": "CÃ³digos y CatÃ¡logos",
  "subcategory": "SAP",
  "question": "Â¿DÃ³nde busco los cÃ³digos de materiales?",
  "priority": "critical",
  "difficulty": "medium",
  "expectedTopics": ["SAP", "cÃ³digo material", "transacciÃ³n", "catÃ¡logo"],
  "expectedDocuments": ["PP-009", "I-006"],
  "createdAt": "2025-10-29T12:00:00Z",
  "createdBy": "Alec",
  "metadata": {
    "source": "Especialistas Salfa",
    "validatedBy": "Sebastian",
    "realWorldUsage": "daily"
  }
}
```

---

### **Template: Response Markdown**

```markdown
# S001-Q001 - Response

**Evaluation:** EVAL-S001-2025-10-29-v1  
**Timestamp:** 2025-10-29 12:30:45  
**Model:** gemini-2.5-flash

---

## Pregunta
Â¿DÃ³nde busco los cÃ³digos de materiales?

---

## Respuesta del Agente

[Texto completo en markdown]

---

## Referencias Utilizadas (3)

### [1] PP-009 Como Imprimir... (81.5% similitud)
- **Chunks:** 2 consolidados
- **Tokens:** 3,468
- **Preview:** MAQSA Abastecimiento...

### [2] I-006 GestiÃ³n Control... (79.2% similitud)
- **Chunks:** 4 consolidados
- **Tokens:** 12,000
- **Preview:** serie, fecha, litros...

### [3] PP-007 Reporte... (76.8% similitud)
- **Chunks:** 2 consolidados
- **Tokens:** 4,000
- **Preview:** menÃº, layout, bodega...

---

## ValidaciÃ³n TÃ©cnica

- âœ… Phantom refs: NO
- âœ… Referencias en rango: SÃ (1-3)
- âœ… Consistencia: 100%
- âœ… Referencias Ãºtiles: SÃ

**Status:** âœ… PASS

---

## MÃ©tricas RAG

- **Search time:** 450ms
- **Top-K chunks:** 10
- **Consolidated docs:** 3
- **Context tokens:** 12,450
- **Context window:** 1.2%

---

## Pendiente

- [ ] EvaluaciÃ³n de experto
- [ ] Feedback sobre precisiÃ³n
- [ ] Sugerencias de mejora
- [ ] AprobaciÃ³n final
```

---

## ğŸ¯ Proceso de EvaluaciÃ³n Completo

### **IteraciÃ³n v1 (Primera EvaluaciÃ³n)**

```
1. PREPARACIÃ“N (5 mins):
   - Crear estructura de evaluaciÃ³n
   - Copiar 66 preguntas S001
   - Copiar 19 preguntas M001
   
2. TESTING TÃ‰CNICO (Nosotros) - Muestra (40 mins):
   - S001: 10 preguntas representativas
   - M001: 4 preguntas representativas
   - Validar sistema funciona
   - Documentar respuestas y referencias
   
3. ENTREGA A EXPERTOS (10 mins):
   - Paquete de evaluaciÃ³n completo
   - GuÃ­as y templates
   - Acceso al sistema
   
4. EVALUACIÃ“N EXPERTOS (Variable - 2-4 horas):
   - Expertos prueban preguntas
   - Califican calidad 1-10
   - Comentan precisiÃ³n tÃ©cnica
   - Aprueban/rechazan/sugieren
   
5. CONSOLIDACIÃ“N (30 mins):
   - Recopilar feedback
   - Generar reporte final
   - Identificar mejoras
   - Planificar v2 si necesario
```

---

### **IteraciÃ³n v2 (Mejoras Aplicadas)**

```
1. APLICAR MEJORAS (Variable):
   - Basado en feedback v1
   - Ajustar extracciÃ³n PDFs
   - Mejorar prompts
   - Agregar documentaciÃ³n faltante
   
2. RE-TESTING (40 mins):
   - Mismas preguntas que fallaron en v1
   - Nuevas preguntas crÃ­ticas
   - Validar mejoras aplicadas
   
3. COMPARACIÃ“N v1 vs v2 (20 mins):
   - Reporte comparativo
   - MÃ©tricas de mejora
   - ROI de cambios
   
4. APROBACIÃ“N FINAL:
   - Si calidad â‰¥ 8.5/10 â†’ ProducciÃ³n
   - Si no â†’ v3
```

---

## ğŸ“Š Reportes Generados

### **Reporte por EvaluaciÃ³n:**

```markdown
# EVAL-S001-2025-10-29-v1 - Summary Report

## Scope
- **Total Questions:** 66
- **Questions Tested:** 10 (15%)
- **Categories Covered:** 10/13 (77%)

## Results
- **Average Quality:** 9.1 / 10
- **Quality Distribution:**
  - Excellent (9-10): 8 preguntas (80%)
  - Good (7-8): 2 preguntas (20%)
  - Poor (<7): 0 preguntas (0%)

## Technical Validation
- **Phantom Refs:** 0 detected (100% clean)
- **Reference Consistency:** 100%
- **System Issues:** 0

## Expert Feedback
- **Evaluators:** Sebastian
- **Responses:** 10
- **Approved:** 9 (90%)
- **Needs Improvement:** 1 (10%)
- **Rejected:** 0 (0%)
- **Average Expert Score:** 9.0 / 10

## Top Performing Questions
1. Q26: Informe petrÃ³leo - 10/10
2. Q32: QuÃ© es ST - 9.5/10
3. Q39: GuÃ­a despacho - 9.5/10

## Areas for Improvement
1. Q04: CÃ³digos servicios - 7/10
   - Feedback: "Falta mencionar transacciÃ³n especÃ­fica"
   - Action: Agregar PP-015 a contexto

## Recommendations
- âœ… Sistema listo para producciÃ³n
- âš ï¸ Considerar agregar 2 documentos faltantes
- âœ… Continuar con evaluaciÃ³n completa (66 preguntas)
```

---

### **Reporte Comparativo:**

```markdown
# Comparative Analysis: S001 vs M001

## Overview
| MÃ©trica | S001 | M001 |
|---------|------|------|
| Questions | 66 | 19 |
| Tested (sample) | 10 (15%) | 4 (21%) |
| Avg Quality | 9.1/10 | 9.25/10 |
| Phantom Refs | 0 | 0 |
| Expert Approval | 90% | 95% |

## Strengths
- **S001:** Procedimientos concretos, pasos accionables
- **M001:** AnÃ¡lisis normativo, referencias legales

## Weaknesses
- **S001:** Algunos cÃ³digos especÃ­ficos no encontrados
- **M001:** Fragmentos vacÃ­os en algunos docs

## Recommendations
- Ambos listos para producciÃ³n
- Mejora continua basada en feedback
```

---

## ğŸš€ Plan de ImplementaciÃ³n

### **Fase 1: Estructura (AHORA - 10 mins)**

```bash
# Crear estructura de directorios
mkdir -p docs/evaluations/{questions,evaluations,iterations,reports}

# Guardar preguntas en JSON
# S001: 66 preguntas
# M001: 19 preguntas

# Crear templates
```

---

### **Fase 2: Testing Muestra (AHORA - 30 mins)**

```bash
# S001: Probar 5-6 preguntas crÃ­ticas
# M001: Ya probadas 4 preguntas

# Generar responses/ y references/ para muestra
# Validar tÃ©cnicamente
# Documentar hallazgos
```

---

### **Fase 3: Entrega a Expertos (LUEGO - 10 mins)**

```bash
# Preparar paquete de evaluaciÃ³n:
- 85 preguntas totales (66 + 19)
- Templates de evaluaciÃ³n
- GuÃ­as de uso
- Sistema funcionando

# Enviar a Sebastian
# Esperar feedback
```

---

### **Fase 4: ConsolidaciÃ³n (DESPUÃ‰S - 30 mins)**

```bash
# Recopilar feedback experto
# Generar reportes finales
# Identificar mejoras para v2
# Planificar prÃ³xima iteraciÃ³n
```

---

## ğŸ“ Archivos a Crear

### **Inmediato:**
1. âœ… `SISTEMA_EVALUACION_AGENTES.md` (este archivo)
2. â³ `questions/S001-questions-v1.json` (66 preguntas)
3. â³ `questions/M001-questions-v1.json` (19 preguntas)
4. â³ `evaluations/EVAL-S001-2025-10-29-v1/` (estructura)
5. â³ `evaluations/EVAL-M001-2025-10-29-v1/` (estructura)

### **Durante Testing:**
6. â³ `responses/Q00X-response.json` (por cada pregunta)
7. â³ `references/Q00X-references.json` (por cada pregunta)
8. â³ `validation-report.md` (tÃ©cnico)

### **Post-Expertos:**
9. â³ `expert-feedback/Q00X-feedback.json` (por feedback)
10. â³ `summary-report.md` (consolidado)
11. â³ `improvements-v1-to-v2.md` (prÃ³xima iteraciÃ³n)

---

## ğŸ“ Beneficios del Sistema

### **Trazabilidad:**
- âœ… Historial completo de cada evaluaciÃ³n
- âœ… ComparaciÃ³n entre versiones
- âœ… Evidencia de mejoras

### **Calidad:**
- âœ… ValidaciÃ³n tÃ©cnica + experta
- âœ… MÃ©tricas objetivas
- âœ… Feedback estructurado

### **Mejora Continua:**
- âœ… IdentificaciÃ³n de patrones
- âœ… PriorizaciÃ³n basada en datos
- âœ… ROI medible

### **Gobernanza:**
- âœ… AprobaciÃ³n formal de expertos
- âœ… Registro de decisiones
- âœ… AuditorÃ­a completa

---

## âœ… PrÃ³ximo Paso

**Voy a:**

1. âœ… Crear archivos JSON con las preguntas
2. âœ… Generar estructura de evaluaciÃ³n
3. âœ… Probar 5-6 preguntas crÃ­ticas de S001
4. âœ… Generar reporte preliminar
5. âœ… Preparar paquete para expertos

**Tiempo:** 30-40 mins  
**Resultado:** Sistema de evaluaciÃ³n completo y funcionando

**Â¿Procedo a crear la estructura completa?** ğŸ¯





