# üöÄ Quick Start Guide - Sistema de Evaluaciones

**Para:** Expertos y Administradores  
**Tiempo:** 10-15 minutos para primera evaluaci√≥n  
**Requisito:** Agente configurado con contexto

---

## ‚ö° 3 Pasos R√°pidos

### 1Ô∏è‚É£ Crear Evaluaci√≥n (3 mins)

```
1. Abrir men√∫ usuario (esquina inferior izquierda)
2. Click "Sistema de Evaluaciones"
3. Click "Nueva Evaluaci√≥n"
4. Seleccionar agente (ej: GESTION BODEGAS GPT)
5. Importar preguntas:
   - Click "Importar JSON"
   - Seleccionar S001-questions-v1.json
   - O agregar manualmente
6. Definir criterios de √©xito:
   - Calidad m√≠nima: 5.0/10
   - Phantom refs: NO
   - Cobertura CRITICAL: 3+
   - Similitud: 70%+
7. Click "Crear Evaluaci√≥n"
```

**Resultado:** Evaluaci√≥n creada con status "Borrador"

---

### 2Ô∏è‚É£ Probar Preguntas (5-10 mins)

```
1. Click en evaluaci√≥n reci√©n creada
2. Ir a tab "Preguntas"
3. Filtrar por "CRITICAL" priority
4. Click "Probar" en primera pregunta
5. Modal se abre:
   - Click "Ejecutar Prueba"
   - Esperar respuesta del agente (30-60s)
   - Sistema muestra respuesta y referencias
6. Evaluar:
   - Mover slider de calidad (1-10)
   - Marcar "Phantom Refs" si detectas
   - Agregar notas (opcional)
7. Click "Guardar Resultado"
8. Repetir para 3-5 preguntas CRITICAL
```

**Resultado:** 3-5 preguntas probadas, stats actualizados

---

### 3Ô∏è‚É£ Revisar y Aprobar (2 mins)

```
1. Ir a tab "Resumen"
2. Revisar m√©tricas:
   - Calidad promedio
   - Phantom refs
   - Cobertura CRITICAL
   - Similitud referencias
3. Verificar criterios de √©xito:
   - ‚úÖ = Verde = Cumplido
   - ‚ùå = Rojo = No cumplido
4. Decisi√≥n:
   - Si todos ‚úÖ ‚Üí Aprobar
   - Si algunos ‚ùå ‚Üí Mejorar agente
```

**Resultado:** Agente aprobado o feedback para mejorar

---

## üìä Ejemplo Real: S001

### Importar Ejemplo S001

```bash
# Desde terminal
cd /Users/alec/salfagpt
npx tsx scripts/import-s001-evaluation.ts
```

**Qu√© importa:**
- ‚úÖ Evaluaci√≥n completa (66 preguntas)
- ‚úÖ 4 resultados de tests (Q001, Q002, Q004, Q009)
- ‚úÖ Criterios de √©xito configurados
- ‚úÖ Stats calculados

**Puedes ver:**
1. Abrir "Sistema de Evaluaciones"
2. Ver "GESTION BODEGAS GPT (S001)"
3. Quality: 9.25/10
4. Status: Completado
5. Explorar tabs: Resumen, Preguntas, Resultados

---

## üéØ Casos de Uso

### Caso 1: Validar Nuevo Agente

**Objetivo:** Antes de compartir con usuarios

**Proceso:**
1. Crear evaluaci√≥n con 10-15 preguntas representativas
2. Incluir mezcla de CRITICAL, HIGH, MEDIUM
3. Probar todas las preguntas
4. Si promedio ‚â• 5/10 y 0 phantom refs ‚Üí Aprobar
5. Compartir con usuarios

**Tiempo:** 30-45 mins

---

### Caso 2: Validaci√≥n R√°pida (Muestra)

**Objetivo:** Verificar sistema funciona bien

**Proceso:**
1. Crear evaluaci√≥n con 5 preguntas CRITICAL
2. Probar las 5
3. Si todas ‚â• 8/10 y 0 phantom refs ‚Üí Sistema validado
4. Aprobar con nota: "Validado con muestra"

**Tiempo:** 15-20 mins

**Ejemplo:** S001 us√≥ este m√©todo (4 preguntas, 9.25/10)

---

### Caso 3: Testing Exhaustivo

**Objetivo:** M√°xima confianza para agente cr√≠tico

**Proceso:**
1. Usar set completo (ej: 66 preguntas S001)
2. Probar todas en sesi√≥n dedicada
3. Documentar cada resultado
4. Generar reporte comprehensivo
5. Aprobar solo si todos criterios cumplidos

**Tiempo:** 3-4 horas

---

## üîç Detecci√≥n de Phantom References

### ¬øQu√© son?

Phantom references = Referencias a documentos que no existen

**Ejemplo:**
```
Referencias mostradas: [1] [2] [3]
Texto menciona: "seg√∫n documento [5]..."
                                  ‚Üë
                          PHANTOM REF!
```

### C√≥mo Detectar

```
1. Expandir "üìö Referencias utilizadas [N]"
2. Contar badges: [1] [2] [3] ... [N]
3. Leer respuesta del agente
4. Buscar n√∫meros: [1], [2], etc.
5. Verificar: ¬øTodos los n√∫meros ‚â§ N?
   - SI ‚Üí No hay phantom refs ‚úÖ
   - NO ‚Üí Hay phantom refs ‚ùå
```

### Auto-Detection

El sistema detecta autom√°ticamente:
- Extrae n√∫meros del texto: \[(\d+)\]
- Compara con total de referencias
- Marca checkbox si detecta

**Siempre verifica manualmente - es cr√≠tico!**

---

## üìà Interpretando Resultados

### Calidad por Rango

| Score | Nivel | Significado | Acci√≥n |
|-------|-------|-------------|--------|
| 9-10/10 | Excelente | Production-ready | ‚úÖ Aprobar |
| 7-8/10 | Muy bueno | Funcional, mejorable | ‚ö†Ô∏è Revisar |
| 5-6/10 | Aceptable | B√°sico, funcional | ‚ö†Ô∏è Mejorar |
| 1-4/10 | Insuficiente | No listo | ‚ùå Rechazar |

### Success Criteria

**M√≠nimo para aprobar:**
- ‚úÖ Calidad promedio ‚â• 5.0/10
- ‚úÖ Phantom refs = 0
- ‚úÖ Al menos 3 CRITICAL probadas
- ‚úÖ Similitud refs ‚â• 70%

**Ideal para producci√≥n:**
- ‚≠ê Calidad promedio ‚â• 8.0/10
- ‚≠ê Phantom refs = 0
- ‚≠ê Todas CRITICAL probadas
- ‚≠ê Similitud refs ‚â• 75%

---

## üõ†Ô∏è Troubleshooting

### Problema: No puedo crear evaluaci√≥n

**Soluci√≥n:**
- Verifica que eres Expert o Admin
- Check permisos en configuraci√≥n de usuario
- Revisa console de browser para errores

### Problema: Test no ejecuta

**Soluci√≥n:**
- Verifica agente tiene contexto activo
- Check que servidor est√° corriendo (localhost:3000)
- Verifica API key de Gemini est√° configurada

### Problema: Referencias no cargan

**Soluci√≥n:**
- Verifica documentos est√°n sincronizados en BigQuery
- Check tabla `document_embeddings` tiene datos
- Revisa configuraci√≥n RAG

### Problema: Stats no actualizan

**Soluci√≥n:**
- Refresh p√°gina
- Check que resultado se guard√≥ (console logs)
- Verifica permissions de Firestore

---

## üìö Recursos

### Documentaci√≥n
- `EVALUATION_SYSTEM.md` - Documentaci√≥n completa
- `S001_TESTING_RESULTS_SUMMARY.md` - Ejemplo de resultados
- `S001-EVALUATION-REPORT-2025-10-29.md` - Reporte detallado

### Archivos de Ejemplo
- `questions/S001-questions-v1.json` - 66 preguntas listas
- `scripts/import-s001-evaluation.ts` - Script de importaci√≥n

### Componentes
- `EvaluationPanel.tsx` - UI principal
- `AgentSharingApprovalModal.tsx` - Aprobaci√≥n para compartir

### APIs
- `POST /api/evaluations` - Crear evaluaci√≥n
- `POST /api/evaluations/:id/test` - Ejecutar test
- `POST /api/evaluations/:id/results` - Guardar resultado
- `GET /api/evaluations/:id/results` - Ver resultados

---

## ‚úÖ Checklist Primera Evaluaci√≥n

### Antes de Empezar
- [ ] Soy Expert o Admin
- [ ] Tengo agente con contexto configurado
- [ ] Tengo preguntas de evaluaci√≥n listas
- [ ] S√© qu√© criterios usar

### Durante Evaluaci√≥n
- [ ] Evaluaci√≥n creada correctamente
- [ ] Preguntas importadas o agregadas
- [ ] Criterios de √©xito definidos
- [ ] Al menos 3 preguntas CRITICAL probadas
- [ ] Cada test documentado con calidad y notas

### Despu√©s de Tests
- [ ] Todos los resultados guardados
- [ ] Stats actualizados correctamente
- [ ] Criterios de √©xito verificados
- [ ] Decisi√≥n de aprobar/rechazar tomada
- [ ] Feedback documentado si aplica

---

## üéì Tips de Expertos

### üí° Tip 1: Muestras Representativas
No necesitas probar todas las 66 preguntas. Una muestra de 10-15 preguntas bien elegidas valida el sistema. S001 us√≥ solo 4 preguntas CRITICAL y tuvo confianza ALTA.

### üí° Tip 2: Prioriza CRITICAL
Enf√≥cate primero en las preguntas CRITICAL. Si estas funcionan bien (8+/10), las dem√°s probablemente tambi√©n.

### üí° Tip 3: Documenta TODO
Escribe notas espec√≠ficas para cada test. Esto ayuda a:
- Entender por qu√© score fue X/10
- Dar feedback al creador del agente
- Mejorar en pr√≥xima versi√≥n

### üí° Tip 4: Usa "Nuevo Chat"
Siempre usa un chat nuevo para cada pregunta. Esto asegura contexto fresco y resultados consistentes.

### üí° Tip 5: Verifica Phantom Refs
Es el error m√°s cr√≠tico. Toma 10 segundos extra verificar, pero previene problemas serios en producci√≥n.

---

## üìû Support

**Problemas t√©cnicos:**
- Check logs en browser console (F12)
- Revisar documentaci√≥n en `docs/`
- Contactar: alec@getaifactory.com

**Preguntas sobre criterios:**
- Revisar S001 como ejemplo
- Consultar con otros expertos
- Referirse a `EVALUATION_SYSTEM.md`

---

**√öltima actualizaci√≥n:** 2025-10-29  
**Versi√≥n:** 1.0.0  
**Status:** ‚úÖ Ready to Use

---

**¬°Empieza con S001 importado como ejemplo! Explora antes de crear tu primera evaluaci√≥n.** üéØ











