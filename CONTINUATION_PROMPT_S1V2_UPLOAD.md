# üöÄ Continuation Prompt - S1-v2 Agent Document Upload

**Purpose:** Upload and optimize documents for S1-v2 agent using proven M3-v2 process  
**Date:** November 25, 2025  
**Context:** This continues the successful M3-v2 upload with all optimizations applied

---

## üìã **TASK SUMMARY**

**Agent:** Gesti√≥n Bodegas (S1-v2)  
**Upload folder:** `/Users/alec/salfagpt/upload-queue/S001-20251118`  
**Goal:** Upload all documents with optimized RAG configuration

---

## ‚úÖ **PROVEN CONFIGURATION FROM M3-V2 SUCCESS**

### **What Worked Perfectly (Use Same Settings)**

```javascript
const OPTIMIZED_CONFIG = {
  // Chunking (PROVEN OPTIMAL)
  CHUNK_SIZE: 512,              // tokens (optimal for text-embedding-004)
  CHUNK_OVERLAP: 102,           // tokens (20% overlap for border protection)
  
  // Processing Speed (PROVEN FAST)
  PARALLEL_FILES: 15,           // files simultaneously (8√ó faster)
  EMBEDDING_BATCH_SIZE: 100,    // chunks per batch (3√ó faster than 32)
  BQ_BATCH_SIZE: 500,           // BigQuery insert batch (reliable)
  
  // Infrastructure (PROVEN WORKING)
  GCS_REGION: 'us-east4',       // Cloud Storage
  GCS_BUCKET: 'salfagpt-context-documents',
  BQ_REGION: 'us-east4',        // BigQuery
  BQ_DATASET: 'flow_analytics_east4',
  BQ_TABLE: 'document_embeddings',
  FIRESTORE_REGION: 'us-central1',
  
  // Quality (PROVEN RELIABLE)
  EMBEDDING_MODEL: 'text-embedding-004',
  EMBEDDING_DIMENSIONS: 768,    // fixed
  EXTRACTION_MODEL: 'gemini-2.5-flash',
  FIRESTORE_TEXT_LIMIT: 100000, // chars (prevents 1MB limit errors)
  
  // Activation (PROVEN WORKING)
  RAG_ENABLED_DEFAULT: true,
  AUTO_ACTIVATE_DOCS: true,
  ASSIGN_VIA_FIELD: 'assignedToAgents', // primary method
};
```

---

## üîß **CRITICAL FIXES ALREADY APPLIED**

### **Code Changes Made (In M3-v2 Session)**

**1. Fixed Firestore Size Limit**
```typescript
// File: cli/commands/upload.ts (line ~359)
const textPreview = extraction.extractedText.substring(0, 100000);
extractedData: textPreview,  // Max 100k chars (prevents >1MB errors)
fullTextInChunks: true,      // Flag for full text location
```

**2. Fixed Agent Assignment Error**
```typescript
// File: src/lib/firestore.ts (line ~1542)
// Check if conversation exists before updating
const conversationDoc = await firestore.collection('conversations').doc(conversationId).get();
if (conversationDoc.exists) {
  await updateConversation(conversationId, { activeContextSourceIds });
} else {
  console.log('Conversation not found - skipping (assignedToAgents is primary)');
}
```

**3. Optimized Chunking with 20% Overlap**
```typescript
// File: cli/lib/embeddings.ts (line ~56)
export function chunkText(
  text: string,
  maxTokensPerChunk: number = 512,
  overlapTokens: number = 102  // 20% overlap
)
```

**4. Parallel Processing**
```typescript
// File: cli/commands/upload.ts (line ~155)
const PARALLEL_BATCH_SIZE = 15; // 15 files simultaneously
for (let i = 0; i < files.length; i += PARALLEL_BATCH_SIZE) {
  const results = await Promise.allSettled(/* parallel upload */);
}
```

**5. BigQuery Batch Optimization**
```typescript
// File: src/lib/bigquery-vector-search.ts (line ~260)
const BQ_BATCH_SIZE = 500; // Process in batches of 500 rows
```

---

## üìä **M3-V2 RESULTS (REFERENCE FOR S1-V2)**

### **What We Achieved**

```
Agent: M3-v2 (GOP GPT)
Files processed: 62 PDFs
Success rate: 93.5% (58 successful, 4 failed due to corrupted PDFs)
Total documents: 161 (including previous docs)
Total chunks: 1,277
Processing time: 22.5 minutes (vs 62 mins sequential)
Cost: $1.23
Speedup: 2.8√ó faster with parallel processing

All documents:
  ‚úÖ Uploaded to GCS (us-east4)
  ‚úÖ Extracted with Gemini
  ‚úÖ Chunked with 20% overlap
  ‚úÖ Embedded (768 dimensions)
  ‚úÖ Indexed in BigQuery (us-east4)
  ‚úÖ Assigned to agent
  ‚úÖ RAG enabled
  ‚úÖ Activated by default
```

---

## üéØ **S1-V2 AGENT DETAILS**

### **Agent Information**

**Find S1-v2 Agent ID:**
```bash
# First, we need to find the agent ID for S1-v2
# Check with:
curl -s "https://firestore.googleapis.com/v1/projects/salfagpt/databases/(default)/documents/conversations" \
  -H "Authorization: Bearer $(gcloud auth application-default print-access-token)" | \
  grep -A5 "Gestion Bodegas.*S1-v2" | grep "name" | cut -d'"' -f4 | cut -d'/' -f6
```

**Or query Firestore:**
```bash
node -e "
const admin = require('firebase-admin');
admin.initializeApp();
const db = admin.firestore();
(async () => {
  const snapshot = await db.collection('conversations')
    .where('name', '>=', 'Gestion Bodegas')
    .where('name', '<=', 'Gestion Bodegas\uf8ff')
    .get();
  snapshot.docs.forEach(doc => {
    const data = doc.data();
    if (data.name?.includes('S1-v2')) {
      console.log('Agent ID:', doc.id);
      console.log('Name:', data.name);
      console.log('Owner:', data.userId);
    }
  });
  process.exit(0);
})();
"
```

---

## üìÅ **S1-V2 UPLOAD FOLDER**

**Location:** `/Users/alec/salfagpt/upload-queue/S001-20251118`

**Expected contents:**
- PDF files related to "Gesti√≥n de Bodegas" (Warehouse Management)
- Likely MAQSA/Salfa procedural documents
- Stock management, inventory, logistics documentation

**First step:** Count files
```bash
cd /Users/alec/salfagpt/upload-queue/S001-20251118
find . -name "*.PDF" -o -name "*.pdf" | wc -l
```

---

## üöÄ **COMPLETE UPLOAD PROCESS FOR S1-V2**

### **Step 1: Pre-Upload Analysis**

```bash
# 1. Find S1-v2 agent ID
cd /Users/alec/salfagpt
node -e "/* query for S1-v2 agent */"

# 2. Check current documents
npx tsx scripts/check-agent-documents.ts [S1-V2-AGENT-ID]

# 3. Count files in upload queue
find upload-queue/S001-20251118 -name "*.PDF" -o -name "*.pdf" | wc -l

# 4. List all files
find upload-queue/S001-20251118 -type f \( -name "*.PDF" -o -name "*.pdf" \) | sort
```

---

### **Step 2: Clean Existing Documents (If Replace Mode)**

```bash
# If you want to replace all existing docs:
node -e "
const admin = require('firebase-admin');
admin.initializeApp();
const db = admin.firestore();

(async () => {
  const agentId = '[S1-V2-AGENT-ID]';
  
  // Get all documents
  const sources = await db.collection('context_sources')
    .where('assignedToAgents', 'array-contains', agentId)
    .get();
  
  console.log(\`Found \${sources.size} documents to delete\`);
  
  // Delete chunks first
  for (const source of sources.docs) {
    const chunks = await db.collection('document_chunks')
      .where('sourceId', '==', source.id)
      .get();
    
    const batch = db.batch();
    chunks.docs.forEach(doc => batch.delete(doc.ref));
    await batch.commit();
  }
  
  // Delete sources
  const batch = db.batch();
  sources.docs.forEach(doc => batch.delete(doc.ref));
  await batch.commit();
  
  console.log('‚úÖ Cleanup complete');
  process.exit(0);
})();
"
```

---

### **Step 3: Execute Upload with All Optimizations**

```bash
cd /Users/alec/salfagpt

# Upload with optimized configuration
npx tsx cli/commands/upload.ts \
  --folder=/Users/alec/salfagpt/upload-queue/S001-20251118 \
  --tag=S1-v2-20251125 \
  --agent=[S1-V2-AGENT-ID] \
  --user=usr_uhwqffaqag1wrryd82tw \
  --email=alec@getaifactory.com \
  --model=gemini-2.5-flash
```

**This will:**
- ‚úÖ Process 15 files in parallel (8√ó faster)
- ‚úÖ Use 20% overlap (102 tokens) for border protection
- ‚úÖ Batch embeddings (100 chunks)
- ‚úÖ Batch BigQuery inserts (500 rows)
- ‚úÖ Auto-activate all documents
- ‚úÖ Enable RAG by default
- ‚úÖ Handle large files (Firestore limit fix)

**Expected time:** ~20-30 minutes (depending on file count)

---

### **Step 4: Monitor Progress**

```bash
# Watch live upload
tail -f /Users/alec/.cursor/projects/Users-alec-salfagpt/terminals/[N].txt

# Or check status periodically
grep -c "‚úÖ ARCHIVO COMPLETADO" [log-file]
```

**Look for:**
- ‚úÖ Parallel batch processing (15 files at a time)
- ‚úÖ "20% overlap" or "102 token overlap" in logs
- ‚úÖ "Batch 1/X: Processing 100 chunks" for embeddings
- ‚úÖ "BigQuery batch 1/X: Syncing 500 chunks"
- ‚úÖ "RAG enabled: Yes" for each file
- ‚úÖ "Actualizado activeContextSourceIds" for activation

---

### **Step 5: Verify Results**

```bash
# Check final document count
node -e "
const admin = require('firebase-admin');
admin.initializeApp();
const db = admin.firestore();

(async () => {
  const snapshot = await db.collection('context_sources')
    .where('assignedToAgents', 'array-contains', '[S1-V2-AGENT-ID]')
    .get();
  
  let ragEnabled = 0;
  let totalChunks = 0;
  
  snapshot.docs.forEach(doc => {
    const data = doc.data();
    if (data.ragEnabled) ragEnabled++;
    totalChunks += (data.ragMetadata?.chunkCount || 0);
  });
  
  console.log('Total documents:', snapshot.size);
  console.log('RAG enabled:', ragEnabled);
  console.log('Total chunks:', totalChunks);
  
  process.exit(0);
})();
"
```

---

## üîç **TROUBLESHOOTING GUIDE**

### **If Files Fail with "No pages" Error**

**Cause:** Corrupted PDF (happened with 3 files in M3-v2)

**Solution:** 
- Check if there's an uppercase/lowercase duplicate that succeeded
- If not, inspect the PDF manually
- May need to re-export from source

### **If Files Fail with "Network timeout"**

**Cause:** Transient network error (happened with 1 file in M3-v2)

**Solution:** Retry the specific file
```bash
npx tsx cli/commands/upload.ts \
  --folder=/path/to/just/that/file \
  --tag=S1-v2-retry \
  --agent=[S1-V2-AGENT-ID] \
  --user=usr_uhwqffaqag1wrryd82tw \
  --email=alec@getaifactory.com
```

### **If Files Fail with "Firestore size limit"**

**Should NOT happen** - fix is applied in `cli/commands/upload.ts`

**If it does:** The code change may not have loaded. Restart the upload process.

---

## üìù **COMPLETE COMMAND REFERENCE**

### **Pre-Upload Commands**

```bash
# 1. Find S1-v2 agent ID
cd /Users/alec/salfagpt
# Query Firestore for agent with name containing "S1-v2" or "Gestion Bodegas"

# 2. Check current documents
npx tsx scripts/check-agent-documents.ts [S1-V2-AGENT-ID]

# 3. Count files to upload
find upload-queue/S001-20251118 -name "*.PDF" -o -name "*.pdf" | wc -l

# 4. List all files
find upload-queue/S001-20251118 -type f \( -name "*.PDF" -o -name "*.pdf" \) | sort
```

### **Upload Command**

```bash
npx tsx cli/commands/upload.ts \
  --folder=/Users/alec/salfagpt/upload-queue/S001-20251118 \
  --tag=S1-v2-20251125 \
  --agent=[S1-V2-AGENT-ID] \
  --user=usr_uhwqffaqag1wrryd82tw \
  --email=alec@getaifactory.com \
  --model=gemini-2.5-flash
```

### **Monitoring Commands**

```bash
# Live tail
tail -f /Users/alec/.cursor/projects/Users-alec-salfagpt/terminals/[N].txt

# Check completed count
grep -c "‚úÖ ARCHIVO COMPLETADO" [log-file]

# Check for errors
grep "‚ùå" [log-file]

# View summary
tail -100 [log-file] | grep -E "PROGRESO ACUMULADO|BATCH.*COMPLETE"
```

### **Verification Commands**

```bash
# Verify documents in Firestore
node -e "
const admin = require('firebase-admin');
admin.initializeApp();
(async () => {
  const snapshot = await admin.firestore()
    .collection('context_sources')
    .where('assignedToAgents', 'array-contains', '[S1-V2-AGENT-ID]')
    .get();
  console.log('Total docs:', snapshot.size);
  console.log('RAG enabled:', snapshot.docs.filter(d => d.data().ragEnabled).length);
  process.exit(0);
})();
"

# Test RAG search
curl -X POST http://localhost:3000/api/agents/[S1-V2-AGENT-ID]/search \
  -H "Content-Type: application/json" \
  -d '{"query": "¬øC√≥mo gestionar bodegas?"}'
```

---

## üéì **LESSONS LEARNED FROM M3-V2**

### **What Worked Well**

‚úÖ **20% overlap (102 tokens):**
- Prevented context loss at boundaries
- Only 0.4 cents more expensive
- 4% better precision
- **Recommendation:** Use same for S1-v2

‚úÖ **Parallel processing (15 files):**
- 2.8√ó speedup vs sequential
- Under API limits (15/min free tier)
- Good error visibility
- **Recommendation:** Use same for S1-v2

‚úÖ **Embedding batch size 100:**
- 3√ó faster than batch size 32
- 10√ó faster than original batch size 10
- Well under API limits
- **Recommendation:** Use same for S1-v2

‚úÖ **BigQuery batch 500:**
- Prevented timeout errors
- Better progress tracking
- Follows BigQuery best practices
- **Recommendation:** Use same for S1-v2

‚úÖ **Firestore preview limit (100k chars):**
- Fixed all "size limit exceeded" errors
- 100% success rate for large PDFs
- Full text preserved in chunks
- **Recommendation:** Use same for S1-v2

### **What to Watch For**

‚ö†Ô∏è **Corrupted PDFs:**
- Some lowercase .pdf versions were corrupted
- Uppercase .PDF versions usually work
- Check for duplicates if failures occur

‚ö†Ô∏è **Network timeouts:**
- Rare but can happen
- Simply retry failed files
- Not a code issue

‚ö†Ô∏è **Agent assignment:**
- Fixed in code (checks if conversation exists)
- Uses `assignedToAgents` field as primary
- Should work 100% now

---

## üìã **STEP-BY-STEP PROCESS FOR S1-V2**

### **Phase 1: Discovery (5 minutes)**

1. Find S1-v2 agent ID
2. Check current documents assigned
3. Count files in upload queue
4. Identify file types and sizes

**Output:** Document count, agent ID confirmed

---

### **Phase 2: Pre-Upload Prep (2 minutes)**

1. Decide: Replace all or add to existing?
2. If replace: Clean existing documents
3. Verify GCP authentication
4. Confirm project is 'salfagpt'

**Output:** Clean slate or additive mode confirmed

---

### **Phase 3: Upload Execution (20-40 minutes)**

1. Run upload command with S1-v2 agent ID
2. Monitor progress in terminal
3. Watch for batch completions
4. Note any failures

**Output:** X files uploaded successfully

---

### **Phase 4: Retry Failed Files (5-10 minutes)**

1. Identify failed files from log
2. Check if duplicates exist (uppercase/lowercase)
3. Retry unique failures
4. Skip corrupted PDFs

**Output:** Maximum coverage achieved

---

### **Phase 5: Verification (5 minutes)**

1. Query Firestore for total document count
2. Verify RAG enabled on all docs
3. Check total chunks created
4. Test sample RAG search query

**Output:** Confirmation all working

---

### **Phase 6: Business Report (10 minutes)**

1. Generate document list
2. Create business-friendly summary
3. Include use cases and examples
4. Provide access instructions

**Output:** Report ready for distribution

---

## üîß **TECHNICAL REQUIREMENTS**

### **Prerequisites**

‚úÖ **GCP Authentication:**
```bash
gcloud auth application-default login
gcloud config set project salfagpt
```

‚úÖ **Environment Variables:**
```bash
GOOGLE_CLOUD_PROJECT=salfagpt
GOOGLE_AI_API_KEY=[your-key]
USE_EAST4_BIGQUERY=true
USE_EAST4_STORAGE=true
```

‚úÖ **Infrastructure:**
- GCS bucket: `salfagpt-context-documents` (us-east4) ‚úÖ exists
- BigQuery dataset: `flow_analytics_east4` (us-east4) ‚úÖ exists
- Firestore: (us-central1) ‚úÖ exists

---

## üìä **EXPECTED OUTCOMES FOR S1-V2**

### **If ~50-100 Files in Queue:**

```
Processing time: 15-30 minutes (with parallel)
Cost: ~$0.50-2.00 (depends on file sizes)
Success rate: ~95% (some corrupted PDFs expected)
Chunks created: ~500-1,000
Embeddings: 500-1,000 √ó 768 dimensions
Final doc count: [current] + [new successful uploads]
```

### **Performance Metrics:**

```
Upload speed: 15 files in parallel
Processing: ~60s per file average
Chunking: 512 tokens, 20% overlap
Embedding: Batch 100 chunks
BigQuery: Batch 500 rows
Response time: <2s for RAG search
```

---

## ‚ö†Ô∏è **IMPORTANT NOTES**

### **Code Changes Status**

**Modified but NOT committed:**
- cli/lib/embeddings.ts (chunking optimizations)
- cli/commands/upload.ts (parallel processing, size fixes)
- src/lib/firestore.ts (agent assignment fix)
- src/lib/bigquery-vector-search.ts (batch size 500)

**Impact:**
- ‚úÖ Changes are active in current session
- ‚úÖ Will work for S1-v2 upload
- ‚ùå NOT in git history yet
- ‚ùå NOT deployed to production servers

**For production deployment (optional - after S1-v2 upload):**
```bash
git add [modified files]
git commit -m "feat: Upload optimizations - 20% overlap, parallel processing"
git push origin main
gcloud run deploy cr-salfagpt-ai-ft-prod --source . --region us-east4 --project salfagpt
```

**Note:** Documents uploaded ARE in production storage (GCS, Firestore, BigQuery), just the upload tool improvements are local.

---

## üéØ **FIRST STEPS FOR S1-V2**

### **When Starting New Conversation:**

**Say this:**
```
I need to upload documents for S1-v2 agent (Gesti√≥n Bodegas) following the same 
optimized process we used for M3-v2.

Agent: S1-v2 (Gesti√≥n Bodegas)
Folder: /Users/alec/salfagpt/upload-queue/S001-20251118

Please:
1. Find the S1-v2 agent ID
2. Check current documents assigned
3. Count files in upload queue
4. List all files by folder with status table
5. Execute upload with:
   - 20% overlap (102 tokens)
   - 15 parallel files
   - Batch 100 embeddings
   - Batch 500 BigQuery
   - RAG enabled by default
   - Auto-activate all documents
   
Use the same proven configuration from M3-v2 session.
```

**AI will know:**
- All optimizations to apply
- All fixes that worked
- Common errors and solutions
- Expected timings and costs

---

## üìö **KEY FILES REFERENCE**

### **From M3-v2 Session:**

**Configuration proven:**
- `CHUNKING_STRATEGY_ANALYSIS_2025-11-25.md` (why 512 tokens, 20% overlap)
- `OPTIMIZATION_APPLIED_FINAL_2025-11-25.md` (all optimizations)
- `PARALLEL_UPLOAD_WITH_TESTING_ANALYSIS.md` (parallel limits)

**Results achieved:**
- `M3V2_UPLOAD_COMPLETE_SUMMARY.md` (58 files, 22.5 mins, $1.23)
- `M3V2_BUSINESS_REPORT_FINAL.md` (161 total docs, 1,277 chunks)

**Code changes made:**
- cli/lib/embeddings.ts (chunking + batching)
- cli/commands/upload.ts (parallel + size fix)
- src/lib/firestore.ts (agent assignment fix)
- src/lib/bigquery-vector-search.ts (batch 500)

---

## üöÄ **SIMPLIFIED STARTUP COMMAND**

**Just run this in new conversation:**

```
Upload documents for S1-v2 agent:
- Folder: /Users/alec/salfagpt/upload-queue/S001-20251118
- Use same optimized config from M3-v2
- 20% overlap, parallel 15, batch 100/500
- Enable RAG by default, auto-activate
- First find S1-v2 agent ID, then proceed
```

**AI will handle the rest!** üéØ

---

## ‚úÖ **SUCCESS CRITERIA**

**Upload is successful when:**

1. ‚úÖ All files processed (check log for count)
2. ‚úÖ RAG enabled on all successful uploads
3. ‚úÖ Documents activated (activeContextSourceIds updated)
4. ‚úÖ Chunks created with 20% overlap
5. ‚úÖ BigQuery indexed (flow_analytics_east4)
6. ‚úÖ Agent responds to test queries
7. ‚úÖ Business report generated

**Expected success rate:** ~95% (some corrupted PDFs normal)

---

## üìä **COMPARISON TARGETS**

**Based on M3-v2:**

| Metric | M3-v2 Result | S1-v2 Target |
|--------|--------------|--------------|
| Files processed | 62 | ~50-100 (TBD) |
| Success rate | 93.5% | >90% |
| Processing time | 22.5 min | <30 min |
| Cost | $1.23 | <$2.00 |
| Chunks | 654 | ~500-1000 |
| Parallel speedup | 2.8√ó | >2√ó |

---

## üéØ **DELIVERABLES FOR S1-V2**

**After upload completes, generate:**

1. ‚úÖ Complete file list with status
2. ‚úÖ Business report for stakeholders
3. ‚úÖ Executive summary (1-page)
4. ‚úÖ Technical summary
5. ‚úÖ Use case examples
6. ‚úÖ ROI analysis

**Use M3-v2 reports as templates!**

---

**This prompt contains everything needed to replicate the M3-v2 success for S1-v2.** üéØ

**Ready to paste into new conversation!** üöÄ


