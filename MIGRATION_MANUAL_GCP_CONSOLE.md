# üîÑ Migraci√≥n Manual via GCP Console (M√°s R√°pido)

**Problema:** bq CLI requiere re-autenticaci√≥n  
**Soluci√≥n:** Usar GCP Console (m√°s simple y directo)  
**Tiempo:** 30-45 minutos

---

## üìã **MIGRACI√ìN PASO A PASO (GCP CONSOLE):**

### **PASO 1: Crear Dataset en us-east4 (2 min)**

1. Abrir: https://console.cloud.google.com/bigquery?project=salfagpt
2. Click en proyecto "salfagpt" (lado izquierdo)
3. Click "‚ãÆ" (3 puntos) ‚Üí "Create dataset"
4. Configurar:
   - **Dataset ID:** `flow_analytics_east4`
   - **Location:** `us-east4` ‚úÖ
   - **Description:** "RAG embeddings - us-east4 (GREEN deployment)"
5. Click "Create dataset"

**‚úÖ Resultado:** Dataset `flow_analytics_east4` creado en us-east4

---

### **PASO 2: Copiar Tabla (10-20 min)**

**Opci√≥n A: Copy Table (M√°s F√°cil)**

1. En BigQuery Console
2. Navegar a: `flow_analytics` > `document_embeddings`
3. Click en la tabla
4. Click "Copy" (arriba a la derecha)
5. En el di√°logo:
   - **Destination project:** salfagpt
   - **Destination dataset:** flow_analytics_east4
   - **Destination table:** document_embeddings
   - **Location:** us-east4
6. Click "Copy"
7. **Esperar 10-20 minutos** (copiar 61K chunks)

**Opci√≥n B: SQL Query (Manual)**

1. Click "Compose new query"
2. Pegar:
```sql
CREATE TABLE `salfagpt.flow_analytics_east4.document_embeddings`
PARTITION BY DATE(created_at)
CLUSTER BY user_id, source_id
AS
SELECT * FROM `salfagpt.flow_analytics.document_embeddings`
```
3. **Location:** Seleccionar "us-east4"
4. Click "Run"
5. Esperar completitud

**‚úÖ Resultado:** 61,565 chunks copiados a us-east4

---

### **PASO 3: Verificar Copia (1 min)**

1. Click "Compose new query"
2. Pegar:
```sql
SELECT 
  'BLUE (us-central1)' as source,
  COUNT(*) as chunks
FROM `salfagpt.flow_analytics.document_embeddings`
UNION ALL
SELECT 
  'GREEN (us-east4)' as source,
  COUNT(*) as chunks  
FROM `salfagpt.flow_analytics_east4.document_embeddings`
```
3. Click "Run"

**‚úÖ Resultado esperado:**
```
BLUE: 61,565
GREEN: 61,565 ‚úì
```

---

### **PASO 4: Crear Vector Index (2 min setup, 20-30 min build)**

1. Navegar a: `flow_analytics_east4` > `document_embeddings`
2. Click pesta√±a "Detalles"
3. Scroll abajo a secci√≥n "√çndices"
4. Si hay opci√≥n "Crear √≠ndice vectorial":
   - Click "Crear √≠ndice vectorial"
   - Columna: `embedding`
   - Distance metric: `COSINE`
   - Index type: `IVF`
   - N√∫mero de listas: `1000`
   - Click "Crear"

**Si NO hay opci√≥n de √≠ndice vectorial:**
```sql
-- Ejecutar esta query en us-east4
CREATE VECTOR INDEX IF NOT EXISTS embedding_cosine_idx
ON `salfagpt.flow_analytics_east4.document_embeddings`(embedding)
OPTIONS(
  distance_type = 'COSINE',
  index_type = 'IVF',
  ivf_options = '{"num_lists": 1000}'
)
```

**‚úÖ Resultado:** √çndice construy√©ndose (background, 20-30 min)

---

### **PASO 5: Actualizar C√≥digo (5 min)**

**Archivo:** `src/lib/bigquery-agent-search.ts`

Cambiar l√≠nea 32:
```typescript
// ‚ùå ANTES:
const DATASET_ID = 'flow_analytics';

// ‚úÖ DESPU√âS:
const DATASET_ID = process.env.USE_EAST4_BIGQUERY === 'true' 
  ? 'flow_analytics_east4'  // GREEN (us-east4)
  : 'flow_analytics';        // BLUE (us-central1) fallback
```

**Otros archivos que usan BigQuery:**
- `src/lib/bigquery-optimized.ts` - Cambiar l√≠nea 24
- `src/lib/bigquery-vector-search.ts` - Cambiar constante DATASET_ID
- `src/lib/rag-indexing.ts` - Si usa dataset hardcoded

**Buscar todos:**
```bash
grep -r "flow_analytics" src/lib/*.ts | grep -v "flow_analytics_"
```

---

### **PASO 6: Test Localhost con GREEN (5 min)**

```bash
# 1. Configurar para usar GREEN
export USE_EAST4_BIGQUERY=true

# 2. Reiniciar servidor
pkill -f "astro dev"
npm run dev

# 3. Test en browser
# - Abrir http://localhost:3000/chat
# - Seleccionar S2-v2
# - Preguntar: "¬øAceite hidr√°ulico Scania P450?"
# - Verificar:
#   ‚úì Respuesta r√°pida (<2s)
#   ‚úì Referencias [1], [2], [3]
#   ‚úì Sin errores en consola

# 4. Verificar logs
# Debe decir: "Dataset: flow_analytics_east4"
```

**‚úÖ Si funciona ‚Üí Continuar a producci√≥n**  
**‚ùå Si falla ‚Üí Quitar env var (vuelve a BLUE)**

---

### **PASO 7: Deploy a Producci√≥n (5 min)**

```bash
# Deploy con feature flag GREEN
gcloud run services update cr-salfagpt-ai-ft-prod \
  --region=us-east4 \
  --update-env-vars="USE_EAST4_BIGQUERY=true"

# Verificar deployment
gcloud run revisions list \
  --service=cr-salfagpt-ai-ft-prod \
  --region=us-east4 \
  --limit=1

# Test producci√≥n
curl https://salfagpt.salfagestion.cl/api/health
```

**‚úÖ Resultado:** Producci√≥n usando GREEN (us-east4)

---

### **PASO 8: Verificar Performance (10 min)**

**En producci√≥n:**
1. Login en https://salfagpt.salfagestion.cl
2. Abrir S2-v2
3. Hacer pregunta: "¬øMedidas seguridad gr√∫a?"
4. **Medir tiempo** (F12 ‚Üí Network ‚Üí messages-stream)

**Esperado:**
- ‚ùå BLUE (antes): ~1.5-2s total
- ‚úÖ GREEN (ahora): **~0.8-1.2s total** ‚ö°‚ö°

**Verificar logs:**
```bash
gcloud logging read \
  "resource.type=cloud_run_revision AND textPayload=~'BigQuery'" \
  --limit=20 \
  --format=json
```

Debe mostrar: `"Dataset: flow_analytics_east4"`

---

### **PASO 9: Monitor 24h (Autom√°tico)**

**M√©tricas a vigilar:**
- Error rate < 0.5%
- Latency p95 < 2s
- No user complaints

**Si todo OK despu√©s de 24h:**
- ‚úÖ Migraci√≥n exitosa
- ‚úÖ Hardcodear GREEN en c√≥digo
- ‚úÖ Deprecar BLUE (no delete a√∫n)

**Si hay problemas:**
```bash
# Rollback inmediato a BLUE
gcloud run services update cr-salfagpt-ai-ft-prod \
  --region=us-east4 \
  --remove-env-vars="USE_EAST4_BIGQUERY"

# Tiempo rollback: <2 minutos
```

---

## üéØ **PLAN ACELERADO (Si tienes prisa):**

### **Migraci√≥n Express (45 min):**

```
1. GCP Console ‚Üí Create dataset (2 min)
2. Copy table (20 min wait)
3. Verify counts (1 min)
4. Update code + test localhost (5 min)
5. Deploy producci√≥n (5 min)
6. Verify working (5 min)
Total: ~40 minutos + 20 min vector index (background)
```

---

## ‚ö†Ô∏è **PROBLEMAS COMUNES:**

### **"Reauthentication required"**
**Soluci√≥n:** Usar GCP Console en vez de CLI ‚úÖ

### **"Cross-region copy not allowed"**
**Soluci√≥n:** Export ‚Üí Cloud Storage ‚Üí Import (ya incluido arriba)

### **"Table already exists"**
**Soluci√≥n:** Usar WRITE_TRUNCATE o DROP primero

---

## üìä **CHECKLIST:**

- [ ] Dataset GREEN creado (us-east4)
- [ ] Tabla copiada (61,565 chunks)
- [ ] Conteos verificados (BLUE = GREEN)
- [ ] C√≥digo actualizado con feature flag
- [ ] Test localhost OK
- [ ] Deploy producci√≥n con GREEN
- [ ] Performance mejorado (medir)
- [ ] Sin errores en logs
- [ ] Users satisfechos

---

## üöÄ **EMPEZAR AHORA:**

**Ir a:** https://console.cloud.google.com/bigquery?project=salfagpt

**Ejecutar pasos 1-3** (30 min)

**Luego actualizar c√≥digo** (5 min)

---

**¬øComenzamos con GCP Console?** üéØ

